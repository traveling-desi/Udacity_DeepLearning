{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 34, 58, 51, 80, 63, 11, 48, 47, 78, 78, 78, 10, 58, 51, 51, 13,\n",
       "       48, 44, 58, 57, 54, 49, 54, 63, 29, 48, 58, 11, 63, 48, 58, 49, 49,\n",
       "       48, 58, 49, 54, 74, 63, 79, 48, 63, 12, 63, 11, 13, 48,  5, 35, 34,\n",
       "       58, 51, 51, 13, 48, 44, 58, 57, 54, 49, 13, 48, 54, 29, 48,  5, 35,\n",
       "       34, 58, 51, 51, 13, 48, 54, 35, 48, 54, 80, 29, 48, 59, 64, 35, 78,\n",
       "       64, 58, 13,  1, 78, 78, 65, 12, 63, 11, 13, 80, 34, 54, 35], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training and validation batches\n",
    "\n",
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the batch size. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    print(x[:100])\n",
    "    print(y[:100])\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print(x[:,:50])\n",
    "    #print(y[:,:50])\n",
    "    print('\\n\\n')\n",
    "    print(n_batches)\n",
    "    print(num_steps)\n",
    "    print(x.shape)\n",
    "    print(chars[n_batches* num_steps:(n_batches* num_steps) +50])\n",
    "    \n",
    "    print('\\n\\n')\n",
    "\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    print(train_x[:,:50])\n",
    "    print(train_y[:,:50])\n",
    "    \n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46 34 58 51 80 63 11 48 47 78 78 78 10 58 51 51 13 48 44 58 57 54 49 54 63\n",
      " 29 48 58 11 63 48 58 49 49 48 58 49 54 74 63 79 48 63 12 63 11 13 48  5 35\n",
      " 34 58 51 51 13 48 44 58 57 54 49 13 48 54 29 48  5 35 34 58 51 51 13 48 54\n",
      " 35 48 54 80 29 48 59 64 35 78 64 58 13  1 78 78 65 12 63 11 13 80 34 54 35]\n",
      "[34 58 51 80 63 11 48 47 78 78 78 10 58 51 51 13 48 44 58 57 54 49 54 63 29\n",
      " 48 58 11 63 48 58 49 49 48 58 49 54 74 63 79 48 63 12 63 11 13 48  5 35 34\n",
      " 58 51 51 13 48 44 58 57 54 49 13 48 54 29 48  5 35 34 58 51 51 13 48 54 35\n",
      " 48 54 80 29 48 59 64 35 78 64 58 13  1 78 78 65 12 63 11 13 80 34 54 35 28]\n",
      "\n",
      "\n",
      "\n",
      "[[46 34 58 51 80 63 11 48 47 78 78 78 10 58 51 51 13 48 44 58 57 54 49 54\n",
      "  63 29 48 58 11 63 48 58 49 49 48 58 49 54 74 63 79 48 63 12 63 11 13 48\n",
      "   5 35]\n",
      " [48 58 57 48 35 59 80 48 28 59 54 35 28 48 80 59 48 29 80 58 13 14 36 48\n",
      "  58 35 29 64 63 11 63 32 48 45 35 35 58 14 48 29 57 54 49 54 35 28 14 48\n",
      "  16  5]\n",
      " [12 54 35  1 78 78 36  3 63 29 14 48 54 80 21 29 48 29 63 80 80 49 63 32\n",
      "   1 48 61 34 63 48 51 11 54  7 63 48 54 29 48 57 58 28 35 54 44 54  7 63\n",
      "  35 80]\n",
      " [35 48 32  5 11 54 35 28 48 34 54 29 48  7 59 35 12 63 11 29 58 80 54 59\n",
      "  35 48 64 54 80 34 48 34 54 29 78 16 11 59 80 34 63 11 48 64 58 29 48 80\n",
      "  34 54]\n",
      " [48 54 80 48 54 29 14 48 29 54 11 42 36 48 29 58 54 32 48 80 34 63 48 59\n",
      "  49 32 48 57 58 35 14 48 28 63 80 80 54 35 28 48  5 51 14 48 58 35 32 78\n",
      "   7 11]\n",
      " [48 23 80 48 64 58 29 78 59 35 49 13 48 64 34 63 35 48 80 34 63 48 29 58\n",
      "  57 63 48 63 12 63 35 54 35 28 48 34 63 48  7 58 57 63 48 80 59 48 80 34\n",
      "  63 54]\n",
      " [34 63 35 48  7 59 57 63 48 44 59 11 48 57 63 14 36 48 29 34 63 48 29 58\n",
      "  54 32 14 48 58 35 32 48 64 63 35 80 48 16 58  7 74 48 54 35 80 59 48 80\n",
      "  34 63]\n",
      " [79 48 16  5 80 48 35 59 64 48 29 34 63 48 64 59  5 49 32 48 11 63 58 32\n",
      "  54 49 13 48 34 58 12 63 48 29 58  7 11 54 44 54  7 63 32 14 48 35 59 80\n",
      "  48 57]\n",
      " [80 48 54 29 35 21 80  1 48 61 34 63 13 21 11 63 48 51 11 59 51 11 54 63\n",
      "  80 59 11 29 48 59 44 48 58 48 29 59 11 80 14 78 16  5 80 48 64 63 21 11\n",
      "  63 48]\n",
      " [48 29 58 54 32 48 80 59 48 34 63 11 29 63 49 44 14 48 58 35 32 48 16 63\n",
      "  28 58 35 48 58 28 58 54 35 48 44 11 59 57 48 80 34 63 48 16 63 28 54 35\n",
      "  35 54]]\n",
      "\n",
      "\n",
      "\n",
      "3970\n",
      "50\n",
      "(10, 198500)\n",
      "[48 58 57 48 35 59 80 48 28 59 54 35 28 48 80 59 48 29 80 58 13 14 36 48 58\n",
      " 35 29 64 63 11 63 32 48 45 35 35 58 14 48 29 57 54 49 54 35 28 14 48 16  5]\n",
      "\n",
      "\n",
      "\n",
      "[[46 34 58 51 80 63 11 48 47 78 78 78 10 58 51 51 13 48 44 58 57 54 49 54\n",
      "  63 29 48 58 11 63 48 58 49 49 48 58 49 54 74 63 79 48 63 12 63 11 13 48\n",
      "   5 35]\n",
      " [48 58 57 48 35 59 80 48 28 59 54 35 28 48 80 59 48 29 80 58 13 14 36 48\n",
      "  58 35 29 64 63 11 63 32 48 45 35 35 58 14 48 29 57 54 49 54 35 28 14 48\n",
      "  16  5]\n",
      " [12 54 35  1 78 78 36  3 63 29 14 48 54 80 21 29 48 29 63 80 80 49 63 32\n",
      "   1 48 61 34 63 48 51 11 54  7 63 48 54 29 48 57 58 28 35 54 44 54  7 63\n",
      "  35 80]\n",
      " [35 48 32  5 11 54 35 28 48 34 54 29 48  7 59 35 12 63 11 29 58 80 54 59\n",
      "  35 48 64 54 80 34 48 34 54 29 78 16 11 59 80 34 63 11 48 64 58 29 48 80\n",
      "  34 54]\n",
      " [48 54 80 48 54 29 14 48 29 54 11 42 36 48 29 58 54 32 48 80 34 63 48 59\n",
      "  49 32 48 57 58 35 14 48 28 63 80 80 54 35 28 48  5 51 14 48 58 35 32 78\n",
      "   7 11]\n",
      " [48 23 80 48 64 58 29 78 59 35 49 13 48 64 34 63 35 48 80 34 63 48 29 58\n",
      "  57 63 48 63 12 63 35 54 35 28 48 34 63 48  7 58 57 63 48 80 59 48 80 34\n",
      "  63 54]\n",
      " [34 63 35 48  7 59 57 63 48 44 59 11 48 57 63 14 36 48 29 34 63 48 29 58\n",
      "  54 32 14 48 58 35 32 48 64 63 35 80 48 16 58  7 74 48 54 35 80 59 48 80\n",
      "  34 63]\n",
      " [79 48 16  5 80 48 35 59 64 48 29 34 63 48 64 59  5 49 32 48 11 63 58 32\n",
      "  54 49 13 48 34 58 12 63 48 29 58  7 11 54 44 54  7 63 32 14 48 35 59 80\n",
      "  48 57]\n",
      " [80 48 54 29 35 21 80  1 48 61 34 63 13 21 11 63 48 51 11 59 51 11 54 63\n",
      "  80 59 11 29 48 59 44 48 58 48 29 59 11 80 14 78 16  5 80 48 64 63 21 11\n",
      "  63 48]\n",
      " [48 29 58 54 32 48 80 59 48 34 63 11 29 63 49 44 14 48 58 35 32 48 16 63\n",
      "  28 58 35 48 58 28 58 54 35 48 44 11 59 57 48 80 34 63 48 16 63 28 54 35\n",
      "  35 54]]\n",
      "[[34 58 51 80 63 11 48 47 78 78 78 10 58 51 51 13 48 44 58 57 54 49 54 63\n",
      "  29 48 58 11 63 48 58 49 49 48 58 49 54 74 63 79 48 63 12 63 11 13 48  5\n",
      "  35 34]\n",
      " [58 57 48 35 59 80 48 28 59 54 35 28 48 80 59 48 29 80 58 13 14 36 48 58\n",
      "  35 29 64 63 11 63 32 48 45 35 35 58 14 48 29 57 54 49 54 35 28 14 48 16\n",
      "   5 80]\n",
      " [54 35  1 78 78 36  3 63 29 14 48 54 80 21 29 48 29 63 80 80 49 63 32  1\n",
      "  48 61 34 63 48 51 11 54  7 63 48 54 29 48 57 58 28 35 54 44 54  7 63 35\n",
      "  80 79]\n",
      " [48 32  5 11 54 35 28 48 34 54 29 48  7 59 35 12 63 11 29 58 80 54 59 35\n",
      "  48 64 54 80 34 48 34 54 29 78 16 11 59 80 34 63 11 48 64 58 29 48 80 34\n",
      "  54 29]\n",
      " [54 80 48 54 29 14 48 29 54 11 42 36 48 29 58 54 32 48 80 34 63 48 59 49\n",
      "  32 48 57 58 35 14 48 28 63 80 80 54 35 28 48  5 51 14 48 58 35 32 78  7\n",
      "  11 59]\n",
      " [23 80 48 64 58 29 78 59 35 49 13 48 64 34 63 35 48 80 34 63 48 29 58 57\n",
      "  63 48 63 12 63 35 54 35 28 48 34 63 48  7 58 57 63 48 80 59 48 80 34 63\n",
      "  54 11]\n",
      " [63 35 48  7 59 57 63 48 44 59 11 48 57 63 14 36 48 29 34 63 48 29 58 54\n",
      "  32 14 48 58 35 32 48 64 63 35 80 48 16 58  7 74 48 54 35 80 59 48 80 34\n",
      "  63 48]\n",
      " [48 16  5 80 48 35 59 64 48 29 34 63 48 64 59  5 49 32 48 11 63 58 32 54\n",
      "  49 13 48 34 58 12 63 48 29 58  7 11 54 44 54  7 63 32 14 48 35 59 80 48\n",
      "  57 63]\n",
      " [48 54 29 35 21 80  1 48 61 34 63 13 21 11 63 48 51 11 59 51 11 54 63 80\n",
      "  59 11 29 48 59 44 48 58 48 29 59 11 80 14 78 16  5 80 48 64 63 21 11 63\n",
      "  48 80]\n",
      " [29 58 54 32 48 80 59 48 34 63 11 29 63 49 44 14 48 58 35 32 48 16 63 28\n",
      "  58 35 48 58 28 58 54 35 48 44 11 59 57 48 80 34 63 48 16 63 28 54 35 35\n",
      "  54 35]]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178650)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the size of this array, we see that we have rows equal to the batch size. When we want to get a batch out of here, we can grab a subset of this array that contains all the rows but has a width equal to the number of steps in the sequence. The first batch looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 34, 58, 51, 80, 63, 11, 48, 47, 78, 78, 78, 10, 58, 51, 51, 13,\n",
       "       48, 44, 58, 57, 54, 49, 54, 63, 29, 48, 58, 11, 63, 48, 58, 49, 49,\n",
       "       48, 58, 49, 54, 74, 63, 79, 48, 63, 12, 63, 11, 13, 48,  5, 35, 34,\n",
       "       58, 51, 51, 13, 48, 44, 58, 57, 54, 49, 13, 48, 54, 29, 48,  5, 35,\n",
       "       34, 58, 51, 51, 13, 48, 54, 35, 48, 54, 80, 29, 48, 59, 64, 35, 78,\n",
       "       64, 58, 13,  1, 78, 78, 65, 12, 63, 11, 13, 80, 34, 54, 35], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46, 34, 58, 51, 80, 63, 11, 48, 47, 78, 78, 78, 10, 58, 51, 51, 13,\n",
       "        48, 44, 58, 57, 54, 49, 54, 63, 29, 48, 58, 11, 63, 48, 58, 49, 49,\n",
       "        48, 58, 49, 54, 74, 63, 79, 48, 63, 12, 63, 11, 13, 48,  5, 35],\n",
       "       [48, 58, 57, 48, 35, 59, 80, 48, 28, 59, 54, 35, 28, 48, 80, 59, 48,\n",
       "        29, 80, 58, 13, 14, 36, 48, 58, 35, 29, 64, 63, 11, 63, 32, 48, 45,\n",
       "        35, 35, 58, 14, 48, 29, 57, 54, 49, 54, 35, 28, 14, 48, 16,  5],\n",
       "       [12, 54, 35,  1, 78, 78, 36,  3, 63, 29, 14, 48, 54, 80, 21, 29, 48,\n",
       "        29, 63, 80, 80, 49, 63, 32,  1, 48, 61, 34, 63, 48, 51, 11, 54,  7,\n",
       "        63, 48, 54, 29, 48, 57, 58, 28, 35, 54, 44, 54,  7, 63, 35, 80],\n",
       "       [35, 48, 32,  5, 11, 54, 35, 28, 48, 34, 54, 29, 48,  7, 59, 35, 12,\n",
       "        63, 11, 29, 58, 80, 54, 59, 35, 48, 64, 54, 80, 34, 48, 34, 54, 29,\n",
       "        78, 16, 11, 59, 80, 34, 63, 11, 48, 64, 58, 29, 48, 80, 34, 54],\n",
       "       [48, 54, 80, 48, 54, 29, 14, 48, 29, 54, 11, 42, 36, 48, 29, 58, 54,\n",
       "        32, 48, 80, 34, 63, 48, 59, 49, 32, 48, 57, 58, 35, 14, 48, 28, 63,\n",
       "        80, 80, 54, 35, 28, 48,  5, 51, 14, 48, 58, 35, 32, 78,  7, 11],\n",
       "       [48, 23, 80, 48, 64, 58, 29, 78, 59, 35, 49, 13, 48, 64, 34, 63, 35,\n",
       "        48, 80, 34, 63, 48, 29, 58, 57, 63, 48, 63, 12, 63, 35, 54, 35, 28,\n",
       "        48, 34, 63, 48,  7, 58, 57, 63, 48, 80, 59, 48, 80, 34, 63, 54],\n",
       "       [34, 63, 35, 48,  7, 59, 57, 63, 48, 44, 59, 11, 48, 57, 63, 14, 36,\n",
       "        48, 29, 34, 63, 48, 29, 58, 54, 32, 14, 48, 58, 35, 32, 48, 64, 63,\n",
       "        35, 80, 48, 16, 58,  7, 74, 48, 54, 35, 80, 59, 48, 80, 34, 63],\n",
       "       [79, 48, 16,  5, 80, 48, 35, 59, 64, 48, 29, 34, 63, 48, 64, 59,  5,\n",
       "        49, 32, 48, 11, 63, 58, 32, 54, 49, 13, 48, 34, 58, 12, 63, 48, 29,\n",
       "        58,  7, 11, 54, 44, 54,  7, 63, 32, 14, 48, 35, 59, 80, 48, 57],\n",
       "       [80, 48, 54, 29, 35, 21, 80,  1, 48, 61, 34, 63, 13, 21, 11, 63, 48,\n",
       "        51, 11, 59, 51, 11, 54, 63, 80, 59, 11, 29, 48, 59, 44, 48, 58, 48,\n",
       "        29, 59, 11, 80, 14, 78, 16,  5, 80, 48, 64, 63, 21, 11, 63, 48],\n",
       "       [48, 29, 58, 54, 32, 48, 80, 59, 48, 34, 63, 11, 29, 63, 49, 44, 14,\n",
       "        48, 58, 35, 32, 48, 16, 63, 28, 58, 35, 48, 58, 28, 58, 54, 35, 48,\n",
       "        44, 11, 59, 57, 48, 80, 34, 63, 48, 16, 63, 28, 54, 35, 35, 54]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by `split_data`. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is a function where I build the graph for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    \n",
    "    print(\"ONE HOT\")\n",
    "    print(inputs.shape)\n",
    "    print(inputs[0:10,0:10])\n",
    "    print(num_classes)\n",
    "    print(x_one_hot.shape)\n",
    "    print(x_one_hot[0:10,0:10, 0:10])\n",
    "    \n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # This makes a list where each element is on step in the sequence\n",
    "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "    \n",
    "    print('FIRST\\n\\n')\n",
    "    print(len(rnn_inputs))\n",
    "    print(rnn_inputs[0])\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    print('OUTPUT\\n\\n')\n",
    "    print(len(outputs))\n",
    "    print(outputs[0])\n",
    "    print('\\n\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    print('SECOND\\n\\n')\n",
    "    print(seq_output.shape)\n",
    "    print(output.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Now connect the RNN putputs to a softmax layer\n",
    "    with  tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    \n",
    "    print('THIRD\\n\\n')\n",
    "    print(logits.shape)\n",
    "    print(preds.shape)\n",
    "    print(y_one_hot.shape)\n",
    "    print(loss)\n",
    "    print(cost)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    \n",
    "    print('End\\n\\n')\n",
    "    \n",
    "    #print(local_dict.keys())\n",
    "    \n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    print(local_dict)\n",
    "    \n",
    "    print(graph)\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to write it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that heps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}_v{validation loss}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46 34 58 51 80 63 11 48 47 78 78 78 10 58 51 51 13 48 44 58 57 54 49 54 63\n",
      " 29 48 58 11 63 48 58 49 49 48 58 49 54 74 63 79 48 63 12 63 11 13 48  5 35\n",
      " 34 58 51 51 13 48 44 58 57 54 49 13 48 54 29 48  5 35 34 58 51 51 13 48 54\n",
      " 35 48 54 80 29 48 59 64 35 78 64 58 13  1 78 78 65 12 63 11 13 80 34 54 35]\n",
      "[34 58 51 80 63 11 48 47 78 78 78 10 58 51 51 13 48 44 58 57 54 49 54 63 29\n",
      " 48 58 11 63 48 58 49 49 48 58 49 54 74 63 79 48 63 12 63 11 13 48  5 35 34\n",
      " 58 51 51 13 48 44 58 57 54 49 13 48 54 29 48  5 35 34 58 51 51 13 48 54 35\n",
      " 48 54 80 29 48 59 64 35 78 64 58 13  1 78 78 65 12 63 11 13 80 34 54 35 28]\n",
      "\n",
      "\n",
      "\n",
      "[[46 34 58 ..., 48  5 35]\n",
      " [58 35 32 ...,  7 34 59]\n",
      " [12 54 80 ..., 54 29 29]\n",
      " ..., \n",
      " [16 63 63 ..., 64 58 13]\n",
      " [58 49 64 ..., 63 48 51]\n",
      " [32 48 34 ..., 78 36  2]]\n",
      "\n",
      "\n",
      "\n",
      "198\n",
      "100\n",
      "(100, 19800)\n",
      "[58 35 32 48 28 58 12 63 48 34 63 11 48 80 64 59 14 48 51 54  7 74 54 35 28\n",
      " 48 59  5 80 48 34 63 11 48 44 58 12 59 11 54 80 63 29 14 48 58 48  7 34 59]\n",
      "\n",
      "\n",
      "\n",
      "[[46 34 58 ..., 48  5 35]\n",
      " [58 35 32 ...,  7 34 59]\n",
      " [12 54 80 ..., 54 29 29]\n",
      " ..., \n",
      " [16 63 63 ..., 64 58 13]\n",
      " [58 49 64 ..., 63 48 51]\n",
      " [32 48 34 ..., 78 36  2]]\n",
      "[[34 58 51 ...,  5 35 34]\n",
      " [35 32 48 ..., 34 59  7]\n",
      " [54 80  7 ..., 29 29 63]\n",
      " ..., \n",
      " [63 63 35 ..., 58 13 29]\n",
      " [49 64 58 ..., 48 51 11]\n",
      " [48 34 59 ..., 36  2 63]]\n",
      "ONE HOT\n",
      "(100, 100)\n",
      "Tensor(\"strided_slice:0\", shape=(10, 10), dtype=int32)\n",
      "83\n",
      "(100, 100, 83)\n",
      "Tensor(\"strided_slice_1:0\", shape=(10, 10, 10), dtype=float32)\n",
      "FIRST\n",
      "\n",
      "\n",
      "100\n",
      "Tensor(\"Squeeze:0\", shape=(100, 83), dtype=float32)\n",
      "\n",
      "\n",
      "\n",
      "OUTPUT\n",
      "\n",
      "\n",
      "100\n",
      "Tensor(\"rnn/multi_rnn_cell/cell_1/dropout/mul:0\", shape=(100, 512), dtype=float32)\n",
      "\n",
      "\n",
      "\n",
      "SECOND\n",
      "\n",
      "\n",
      "(100, 51200)\n",
      "(10000, 512)\n",
      "THIRD\n",
      "\n",
      "\n",
      "(10000, 83)\n",
      "(10000, 83)\n",
      "(100, 100, 83)\n",
      "Tensor(\"Reshape_4:0\", shape=(10000,), dtype=float32)\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "End\n",
      "\n",
      "\n",
      "{'final_state': (LSTMStateTuple(c=<tf.Tensor 'rnn/multi_rnn_cell_99/cell_0/basic_lstm_cell/add_1:0' shape=(100, 512) dtype=float32>, h=<tf.Tensor 'rnn/multi_rnn_cell_99/cell_0/basic_lstm_cell/mul_2:0' shape=(100, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/multi_rnn_cell_99/cell_1/basic_lstm_cell/add_1:0' shape=(100, 512) dtype=float32>, h=<tf.Tensor 'rnn/multi_rnn_cell_99/cell_1/basic_lstm_cell/mul_2:0' shape=(100, 512) dtype=float32>)), 'num_steps': 100, 'lstm_size': 512, 'softmax_w': <tensorflow.python.ops.variables.Variable object at 0x1116284a8>, 'cell': <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object at 0x10d106f98>, 'keep_prob': <tf.Tensor 'keep_prob:0' shape=<unknown> dtype=float32>, 'logits': <tf.Tensor 'add:0' shape=(10000, 83) dtype=float32>, 'learning_rate': 0.001, 'loss': <tf.Tensor 'Reshape_4:0' shape=(10000,) dtype=float32>, 'drop': <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.DropoutWrapper object at 0x10d106eb8>, 'export_nodes': ['inputs', 'targets', 'initial_state', 'final_state', 'keep_prob', 'cost', 'preds', 'optimizer'], 'tvars': [<tensorflow.python.ops.variables.Variable object at 0x111ecc8d0>, <tensorflow.python.ops.variables.Variable object at 0x111ecc710>, <tensorflow.python.ops.variables.Variable object at 0x111f1d6a0>, <tensorflow.python.ops.variables.Variable object at 0x111f1d780>, <tensorflow.python.ops.variables.Variable object at 0x1116284a8>, <tensorflow.python.ops.variables.Variable object at 0x111628710>], 'grad_clip': 5, 'cost': <tf.Tensor 'Mean:0' shape=() dtype=float32>, 'train_op': <tensorflow.python.training.adam.AdamOptimizer object at 0x1147b0860>, 'grads': [<tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_0:0' shape=(595, 2048) dtype=float32>, <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_1:0' shape=(2048,) dtype=float32>, <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_2:0' shape=(1024, 2048) dtype=float32>, <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_3:0' shape=(2048,) dtype=float32>, <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_4:0' shape=(512, 83) dtype=float32>, <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_5:0' shape=(83,) dtype=float32>], 'outputs': [<tf.Tensor 'rnn/multi_rnn_cell/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_1/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_2/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_3/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_4/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_5/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_6/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_7/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_8/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_9/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_10/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_11/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_12/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_13/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_14/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_15/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_16/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_17/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_18/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_19/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_20/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_21/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_22/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_23/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_24/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_25/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_26/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_27/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_28/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_29/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_30/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_31/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_32/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_33/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_34/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_35/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_36/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_37/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_38/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_39/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_40/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_41/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_42/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_43/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_44/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_45/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_46/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_47/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_48/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_49/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_50/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_51/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_52/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_53/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_54/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_55/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_56/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_57/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_58/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_59/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_60/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_61/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_62/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_63/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_64/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_65/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_66/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_67/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_68/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_69/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_70/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_71/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_72/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_73/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_74/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_75/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_76/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_77/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_78/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_79/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_80/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_81/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_82/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_83/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_84/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_85/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_86/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_87/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_88/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_89/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_90/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_91/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_92/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_93/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_94/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_95/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_96/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_97/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_98/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>, <tf.Tensor 'rnn/multi_rnn_cell_99/cell_1/dropout/mul:0' shape=(100, 512) dtype=float32>], 'initial_state': (LSTMStateTuple(c=<tf.Tensor 'zeros:0' shape=(100, 512) dtype=float32>, h=<tf.Tensor 'zeros_1:0' shape=(100, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'zeros_2:0' shape=(100, 512) dtype=float32>, h=<tf.Tensor 'zeros_3:0' shape=(100, 512) dtype=float32>)), 'preds': <tf.Tensor 'predictions:0' shape=(10000, 83) dtype=float32>, '_': <tf.Tensor 'global_norm/global_norm:0' shape=() dtype=float32>, 'sampling': False, 'Graph': <class '__main__.Graph'>, 'inputs': <tf.Tensor 'inputs:0' shape=(100, 100) dtype=int32>, 'lstm': <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x105966ef0>, 'output': <tf.Tensor 'Reshape:0' shape=(10000, 512) dtype=float32>, 'y_reshaped': <tf.Tensor 'Reshape_1:0' shape=(10000, 83) dtype=float32>, 'state': (LSTMStateTuple(c=<tf.Tensor 'rnn/multi_rnn_cell_99/cell_0/basic_lstm_cell/add_1:0' shape=(100, 512) dtype=float32>, h=<tf.Tensor 'rnn/multi_rnn_cell_99/cell_0/basic_lstm_cell/mul_2:0' shape=(100, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/multi_rnn_cell_99/cell_1/basic_lstm_cell/add_1:0' shape=(100, 512) dtype=float32>, h=<tf.Tensor 'rnn/multi_rnn_cell_99/cell_1/basic_lstm_cell/mul_2:0' shape=(100, 512) dtype=float32>)), 'targets': <tf.Tensor 'targets:0' shape=(100, 100) dtype=int32>, 'x_one_hot': <tf.Tensor 'one_hot:0' shape=(100, 100, 83) dtype=float32>, 'seq_output': <tf.Tensor 'concat:0' shape=(100, 51200) dtype=float32>, 'batch_size': 100, 'rnn_inputs': [<tf.Tensor 'Squeeze:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_1:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_2:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_3:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_4:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_5:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_6:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_7:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_8:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_9:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_10:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_11:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_12:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_13:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_14:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_15:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_16:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_17:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_18:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_19:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_20:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_21:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_22:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_23:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_24:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_25:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_26:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_27:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_28:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_29:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_30:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_31:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_32:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_33:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_34:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_35:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_36:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_37:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_38:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_39:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_40:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_41:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_42:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_43:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_44:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_45:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_46:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_47:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_48:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_49:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_50:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_51:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_52:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_53:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_54:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_55:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_56:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_57:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_58:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_59:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_60:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_61:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_62:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_63:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_64:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_65:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_66:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_67:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_68:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_69:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_70:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_71:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_72:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_73:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_74:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_75:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_76:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_77:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_78:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_79:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_80:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_81:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_82:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_83:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_84:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_85:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_86:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_87:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_88:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_89:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_90:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_91:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_92:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_93:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_94:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_95:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_96:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_97:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_98:0' shape=(100, 83) dtype=float32>, <tf.Tensor 'Squeeze_99:0' shape=(100, 83) dtype=float32>], 'y_one_hot': <tf.Tensor 'one_hot_1:0' shape=(100, 100, 83) dtype=float32>, 'optimizer': <tf.Operation 'Adam' type=NoOp>, 'num_classes': 83, 'softmax_b': <tensorflow.python.ops.variables.Variable object at 0x111628710>, 'num_layers': 2}\n",
      "Graph(inputs=<tf.Tensor 'inputs:0' shape=(100, 100) dtype=int32>, targets=<tf.Tensor 'targets:0' shape=(100, 100) dtype=int32>, initial_state=(LSTMStateTuple(c=<tf.Tensor 'zeros:0' shape=(100, 512) dtype=float32>, h=<tf.Tensor 'zeros_1:0' shape=(100, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'zeros_2:0' shape=(100, 512) dtype=float32>, h=<tf.Tensor 'zeros_3:0' shape=(100, 512) dtype=float32>)), final_state=(LSTMStateTuple(c=<tf.Tensor 'rnn/multi_rnn_cell_99/cell_0/basic_lstm_cell/add_1:0' shape=(100, 512) dtype=float32>, h=<tf.Tensor 'rnn/multi_rnn_cell_99/cell_0/basic_lstm_cell/mul_2:0' shape=(100, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/multi_rnn_cell_99/cell_1/basic_lstm_cell/add_1:0' shape=(100, 512) dtype=float32>, h=<tf.Tensor 'rnn/multi_rnn_cell_99/cell_1/basic_lstm_cell/mul_2:0' shape=(100, 512) dtype=float32>)), keep_prob=<tf.Tensor 'keep_prob:0' shape=<unknown> dtype=float32>, cost=<tf.Tensor 'Mean:0' shape=() dtype=float32>, preds=<tf.Tensor 'predictions:0' shape=(10000, 83) dtype=float32>, optimizer=<tf.Operation 'Adam' type=NoOp>)\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4179 15.6456 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3684 11.9987 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.1670 12.8778 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.3767 13.9771 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.3524 13.9503 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.2713 12.0820 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.1878 16.2534 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 4.1097 11.2606 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 4.0379 10.6350 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.9765 11.1981 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.9238 10.5819 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.8783 10.5515 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.8381 10.4657 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.8034 10.6349 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.7718 10.2741 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.7427 10.2899 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.7156 10.1349 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.6930 10.1690 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.6709 10.1356 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.6489 10.0597 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.6297 10.1799 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.6125 10.4955 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.5965 10.0057 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.5816 10.4049 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.5674 9.9714 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.5544 10.7548 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.5424 10.6780 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.5304 12.3638 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.5192 10.3537 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.5091 10.4128 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.5002 10.1954 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.4908 10.1748 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.4818 10.0944 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.4738 10.0367 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.4655 9.9766 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.4581 10.2982 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.4504 10.2511 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.4432 12.1122 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.4360 10.7846 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.4293 11.4769 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.4228 11.2918 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.4167 10.7507 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.4107 11.5656 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.4048 10.5326 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.3990 10.4881 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.3940 10.5310 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.3892 11.5985 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.3848 12.7447 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.3804 12.3315 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.3761 10.3625 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.3718 10.6054 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.3675 10.1998 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.3636 12.9770 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.3595 14.3700 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.3557 13.5823 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.3517 12.3103 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.3481 10.7714 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.3446 10.7253 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.3409 11.2362 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.3377 11.0567 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.3345 10.2871 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.3317 10.8058 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.3291 13.2034 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.3258 12.2582 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.3228 11.8829 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.3202 13.4485 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.3175 11.4885 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.3143 14.3276 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.3115 12.7226 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.3091 10.5992 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.3064 10.3588 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.3042 14.1581 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.3017 11.6157 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.2994 12.7222 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.2971 10.9858 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.2950 13.3365 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.2928 10.6866 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.2906 11.3378 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.2884 11.0430 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.2860 12.5082 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.2838 12.7570 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.2818 11.3690 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.2798 10.4588 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.2777 10.4328 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.2755 10.3636 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.2734 10.3548 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.2726 16.2586 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.2752 12.5572 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.2780 11.2075 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.2799 10.2662 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.2808 10.1856 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.2808 10.1805 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.2796 11.0284 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.2784 17.7031 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.2769 14.4487 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.2755 10.7680 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.2741 11.8415 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.2727 15.1110 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.2712 17.4982 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.2699 14.3088 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.2685 10.7617 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.2671 10.3200 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.2657 10.2091 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.2642 10.2499 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.2627 10.4751 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.2613 11.4059 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.2596 10.4630 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.2580 10.6646 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.2566 11.0489 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.2548 10.9625 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.2533 12.3057 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.2517 10.3649 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.2500 10.4971 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.2484 10.2635 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.2467 11.0364 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.2450 13.2098 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.2434 12.8074 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.2419 12.4944 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.2404 10.5251 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.2387 10.1884 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.2372 10.1896 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.2356 10.1947 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.2340 10.1347 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.2324 10.5876 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.2307 11.6103 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.2288 10.3133 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.2270 13.7295 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.2255 12.0592 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.2241 10.7171 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.2225 10.2950 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.2210 10.3653 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.2194 10.4280 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.2179 10.1947 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.2162 10.3767 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.2144 10.3027 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.2126 10.1783 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.2109 10.3294 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.2090 10.2789 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.2074 10.2861 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.2055 10.1969 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.2037 10.2526 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.2017 11.0496 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.1997 10.3023 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.1977 10.2328 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.1957 10.3711 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.1938 11.7409 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.1919 14.9074 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.1900 11.1078 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.1878 15.7349 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.1857 16.0729 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.1838 13.3018 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.1820 10.5133 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.1799 11.1846 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.1778 10.3069 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.1756 10.2617 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.1733 10.5105 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.1710 10.3162 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.1687 10.3309 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.1661 10.2830 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.1638 10.3937 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.1614 10.2910 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.1589 10.4844 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.1563 10.3194 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.1539 10.1864 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.1515 10.2715 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.1490 10.2514 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.1464 10.2397 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.1439 10.3042 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.1414 10.1866 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 3.1388 10.1860 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 3.1363 10.2171 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 3.1339 10.2926 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 3.1315 10.3348 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 3.1291 10.3993 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 3.1266 10.2404 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 3.1240 10.4571 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 3.1212 10.5347 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 3.1184 11.0131 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.6901 10.2030 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.6451 10.2393 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.6322 10.2354 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.6263 10.2430 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.6206 10.4740 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.6144 10.2638 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.6112 10.3983 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.6085 10.3538 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.6067 10.3772 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.6030 10.4358 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.5988 10.4130 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.5960 10.5907 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.5933 12.0224 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.5927 13.2531 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.5893 10.3982 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.5867 10.3131 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.5839 10.2381 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.5839 13.4457 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.5815 11.2330 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.5783 10.5111 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.5756 11.4983 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.5747 11.4516 sec/batch\n",
      "Validation loss: 2.434 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.5732 14.2009 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.5702 12.1331 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.5671 11.3687 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.5646 12.8438 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.5620 11.7746 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.5591 12.2081 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.5570 11.9027 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.5550 12.4706 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.5535 13.2359 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.5507 12.9856 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.5479 12.5523 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.5458 10.7265 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.5434 12.2143 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.5413 10.3563 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.5391 10.3337 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.5362 10.3643 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.5339 10.4783 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.5319 10.5219 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.5294 10.3513 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.5272 16.5177 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.5249 16.3291 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.5226 17.3938 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.5203 15.8391 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.5176 16.0158 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.5160 14.9849 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.5141 16.5660 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.5121 10.9176 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.5106 10.4008 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.5085 10.2316 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.5068 10.2386 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.5048 10.1692 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.5029 10.2002 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.5009 10.1386 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.4993 10.1471 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.4975 10.1698 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.4956 10.1559 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.4939 10.3755 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.4926 12.5259 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.4910 11.6537 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.4897 11.1181 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.4885 11.3974 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.4868 10.6384 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.4851 10.6977 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.4838 11.1577 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.4823 10.2971 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.4803 10.2462 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.4785 10.1055 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.4771 10.0558 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.4757 10.0609 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.4745 10.1694 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.4731 10.0773 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.4714 10.0417 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.4698 10.1043 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.4687 10.0207 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.4671 10.1244 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.4658 10.0258 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.4641 10.0691 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.4627 10.1002 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.4611 10.0643 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.4598 10.0182 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.4582 10.1430 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.4565 10.0847 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.4545 10.0919 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.4530 10.1086 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.4516 10.0661 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.4502 10.0281 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.4486 10.1991 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.4473 10.3283 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.4458 9.9848 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.4445 10.0799 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.4430 10.0306 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.4415 10.0581 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.4398 9.9564 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.4383 10.0146 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.4371 10.0788 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.4357 9.9855 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.4341 10.2130 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.4326 10.1263 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.4314 10.1111 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.4301 10.0530 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.4285 10.0877 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.4271 10.2598 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.4257 10.2464 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.4243 10.9709 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.4229 12.9357 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.4219 10.7205 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.4207 11.5379 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.4194 12.1168 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.4182 10.2012 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.4170 10.1611 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.4157 11.1285 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.4143 10.3905 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.4129 10.2542 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.4113 10.0952 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.4100 10.2128 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.4087 10.0919 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.4075 10.0049 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.4063 10.0875 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.4052 10.1037 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.4038 10.3261 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.4025 10.9891 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.4014 11.1118 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.4003 13.1755 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.3988 10.3803 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.3977 10.2789 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.3966 10.4933 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.3955 10.5636 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.3944 10.0867 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.3931 9.9870 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.3917 9.9366 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.3906 9.9656 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.3895 10.0111 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.3883 10.0454 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.3872 9.9779 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.3861 9.8925 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.3850 9.9841 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.3842 9.9946 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.3830 10.5492 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.3820 11.0723 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.3808 11.6146 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.3797 10.9670 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.3786 10.3897 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.3775 10.4167 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.3765 10.2874 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.3755 10.2738 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.3745 9.9870 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.3734 10.0363 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.3722 9.9780 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.3713 10.0234 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.3704 11.1480 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.3694 10.5727 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.3684 10.7632 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.3672 11.5819 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.3662 10.5497 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.3650 10.4249 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.3639 10.3092 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.3626 10.3020 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.3619 10.5575 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.3609 12.0116 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.3598 10.8011 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.3587 10.7425 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.3576 10.5848 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.3566 10.7073 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.3556 10.5931 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.3545 10.6205 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.3536 10.8932 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.3526 10.5137 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.3515 10.3861 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.3503 10.3948 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.3493 10.3341 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.3484 10.7329 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.3474 10.1707 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.3465 10.1515 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.3455 10.1343 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.3444 10.8271 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.3434 10.0940 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.2335 10.0280 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.1807 10.1131 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.1691 10.0594 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.1606 10.0073 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.1580 10.0735 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.1515 9.9590 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.1519 9.9021 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.1523 9.9968 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.1544 10.1786 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.1522 10.0070 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.1494 10.2175 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.1474 10.1840 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.1469 10.0615 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.1487 10.0449 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.1476 9.9988 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.1462 9.9995 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.1447 10.0396 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.1467 10.0870 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.1460 9.9494 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.1448 9.9995 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.1437 10.3785 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.1446 10.5540 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.1435 10.0911 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.1418 10.0173 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.1410 10.0684 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.1395 10.2445 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.1380 10.1200 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.1374 10.0120 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.1375 9.9026 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.1368 9.9632 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.1360 9.9356 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.1348 9.9874 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.1337 10.0793 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.1335 10.0354 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.1322 10.0697 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.1314 10.0766 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.1306 10.1552 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.1287 10.1994 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.1270 10.2023 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.1254 10.0306 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.1242 10.1083 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.1235 10.0603 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.1222 10.0951 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.1207 9.9803 sec/batch\n",
      "Validation loss: 1.98431 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.1207 11.0639 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.1186 11.4754 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.1178 10.0716 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.1164 9.6611 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.1156 10.3285 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.1154 10.8482 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.1141 10.2534 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.1141 10.8732 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.1130 11.6821 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.1122 10.6529 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.1112 10.9554 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.1105 10.1786 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.1100 10.0305 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.1093 9.9954 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.1081 10.1291 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.1078 9.9747 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.1069 10.1647 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.1067 10.1512 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.1064 10.1695 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.1057 10.1035 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.1049 10.0379 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.1046 10.0502 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.1039 10.0523 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.1028 10.1209 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.1019 10.0743 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.1011 9.9787 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.1008 10.0642 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.1002 11.9816 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.0998 10.1704 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.0989 10.5611 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.0981 10.1935 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.0978 10.0918 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.0970 10.0086 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.0964 10.0346 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.0953 10.1104 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.0945 10.1692 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.0934 10.3986 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.0929 10.1207 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.0917 10.2069 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.0909 10.2448 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.0897 10.0331 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.0888 10.1134 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.0881 10.1073 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.0872 10.2423 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.0862 10.1313 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.0856 9.9858 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.0847 10.0822 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.0839 10.0600 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.0828 10.0191 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.0818 10.2000 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.0809 10.1048 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.0800 10.0142 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.0794 9.9470 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.0784 10.1120 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.0773 14.1501 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.0762 10.5441 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.0755 10.3435 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.0749 10.3218 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.0740 10.6640 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.0731 10.3722 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.0722 10.0561 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.0715 9.9710 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.0708 10.4042 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.0702 10.0116 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.0696 10.0302 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.0689 10.0525 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.0682 10.0560 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.0675 10.0790 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.0668 9.9393 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.0660 10.0709 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.0651 9.9916 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.0642 10.1611 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.0635 10.2541 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.0628 10.0303 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.0621 10.0315 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.0616 10.6539 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.0610 10.1142 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.0602 10.3595 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.0594 10.4154 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.0589 10.0131 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.0582 9.9854 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.0572 9.9671 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.0566 10.0023 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.0561 9.9438 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.0555 9.9863 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.0548 10.0789 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.0540 10.0536 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.0532 10.0217 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.0527 9.9700 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.0521 9.9682 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.0514 9.9238 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.0508 10.0046 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.0503 10.0181 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.0496 9.9950 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.0493 9.9540 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.0487 9.9486 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.0482 9.9683 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.0475 10.6706 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.0469 12.0566 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.0464 10.0205 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.0457 9.9646 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.0453 10.1906 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.0447 10.2795 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.0444 10.2885 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.0437 9.9908 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.0431 9.9794 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.0425 10.4951 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.0421 10.7147 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.0416 10.3522 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.0411 10.3862 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.0404 10.5051 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.0399 9.9783 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.0393 9.9912 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.0387 10.0474 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.0380 10.3022 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.0376 10.2517 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.0372 10.0406 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.0366 9.9976 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.0361 10.0366 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.0356 10.1431 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.0350 10.0509 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.0343 11.2280 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.0339 13.5081 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.0336 11.0031 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.0330 10.3953 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.0324 10.1976 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.0317 10.0184 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.0310 10.0906 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.0306 10.4299 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.0301 10.3642 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.0295 10.3299 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.0290 10.0791 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.0283 11.0657 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.0278 12.3237 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 2.0085 12.0986 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 1.9572 10.1992 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 1.9416 11.1611 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 1.9333 10.1721 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 1.9292 11.4688 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 1.9193 13.1254 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 1.9204 10.6846 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 1.9197 12.7213 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 1.9220 11.8634 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 1.9205 12.0239 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 1.9173 11.9626 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 1.9153 10.6314 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 1.9152 10.4720 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 1.9178 10.2101 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 1.9165 10.5474 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 1.9144 11.6596 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 1.9142 11.3221 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 1.9158 11.4851 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 1.9155 11.1160 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 1.9161 10.2776 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 1.9150 10.8081 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 1.9163 10.8008 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 1.9154 11.1677 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 1.9142 11.3575 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 1.9136 11.6227 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 1.9123 10.2224 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 1.9110 12.0064 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 1.9109 13.6701 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 1.9115 11.0274 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 1.9108 10.2262 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 1.9104 10.1825 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 1.9092 11.6809 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 1.9089 11.3334 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 1.9094 11.2995 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 1.9089 11.8358 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 1.9082 13.0064 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 1.9072 10.4890 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 1.9055 10.7207 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 1.9036 14.1412 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 1.9023 15.1571 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 1.9012 12.3340 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 1.9009 17.4169 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 1.9000 10.8146 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 1.8987 10.3696 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 1.8983 11.1077 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 1.8966 13.5766 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 1.8959 12.3847 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 1.8947 11.9911 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 1.8941 11.7260 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 1.8945 10.8966 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 1.8933 10.1198 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 1.8938 10.0783 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 1.8931 10.1005 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 1.8925 10.6387 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 1.8917 10.3937 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 1.8914 10.4042 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 1.8913 11.0812 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 1.8907 10.5622 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 1.8898 10.4086 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 1.8899 10.2594 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 1.8894 10.1041 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 1.8897 10.0479 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 1.8896 10.0817 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 1.8895 10.0976 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 1.8890 10.0567 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 1.8889 10.0063 sec/batch\n",
      "Validation loss: 1.75336 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 1.8892 11.0563 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 1.8884 9.7925 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 1.8879 9.6904 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 1.8875 9.7623 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 1.8874 10.2137 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 1.8871 9.9895 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 1.8870 9.9931 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 1.8863 10.0614 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 1.8858 10.0361 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 1.8857 9.9381 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 1.8851 10.1102 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 1.8849 10.0929 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 1.8839 10.0819 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 1.8834 10.0022 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 1.8825 10.0424 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 1.8824 10.0434 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 1.8815 10.2061 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 1.8811 10.0426 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 1.8802 10.0790 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 1.8795 10.1315 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 1.8789 10.3667 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 1.8783 10.3585 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 1.8774 11.0686 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 1.8771 10.6312 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 1.8765 10.9840 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 1.8760 10.7576 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 1.8752 10.2192 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 1.8745 10.8733 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 1.8737 11.5082 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 1.8732 11.6471 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 1.8727 15.5851 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 1.8719 10.6117 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 1.8711 11.4102 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 1.8702 10.4617 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 1.8698 10.1119 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 1.8693 10.0270 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 1.8687 9.9843 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 1.8681 9.9708 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 1.8674 9.9101 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 1.8669 10.0008 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 1.8664 9.9738 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 1.8660 9.9309 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 1.8657 9.9915 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 1.8652 10.0143 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 1.8648 9.9821 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 1.8641 10.0931 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 1.8636 10.0739 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 1.8631 10.1729 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 1.8625 9.9919 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 1.8617 10.1107 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 1.8612 10.2229 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 1.8607 10.1137 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 1.8602 9.9670 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 1.8598 9.9832 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 1.8594 10.0411 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 1.8587 10.1195 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 1.8580 10.1498 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 1.8578 10.2213 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 1.8574 10.0764 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 1.8567 10.1226 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 1.8564 10.0041 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 1.8560 9.9774 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 1.8555 10.9853 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 1.8550 11.7560 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 1.8543 10.3517 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 1.8538 11.5274 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 1.8534 10.0890 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 1.8531 10.5777 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 1.8526 10.0672 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 1.8523 9.9824 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 1.8520 10.0150 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 1.8517 9.9867 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 1.8514 9.9916 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 1.8509 9.9227 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 1.8508 10.0002 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 1.8504 9.9933 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 1.8499 10.0527 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 1.8495 10.0681 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 1.8490 10.0872 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 1.8487 10.0150 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 1.8484 10.1186 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 1.8481 10.1290 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 1.8477 10.1636 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 1.8473 10.0577 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 1.8467 10.2115 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 1.8465 12.6463 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 1.8462 11.4108 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 1.8458 10.0418 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 1.8454 10.1553 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 1.8451 10.1691 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 1.8448 10.1657 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 1.8443 10.3329 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 1.8437 10.8254 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 1.8435 10.0755 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 1.8432 10.3398 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 1.8428 9.9981 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 1.8426 10.1926 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 1.8422 10.0767 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 1.8418 10.3229 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 1.8413 10.1999 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 1.8410 10.1370 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 1.8409 10.0591 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 1.8405 9.9728 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 1.8400 11.2918 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 1.8396 10.4714 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 1.8391 12.5581 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 1.8388 10.3672 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 1.8385 11.0252 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 1.8381 12.0562 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 1.8377 10.3893 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 1.8372 10.0304 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 1.8369 9.9806 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 1.8627 10.0141 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 1.8087 10.1617 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 1.7922 10.0645 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 1.7853 10.0417 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 1.7782 10.1158 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 1.7670 10.1243 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 1.7660 10.0705 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 1.7641 10.0505 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 1.7662 10.0549 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 1.7653 10.4663 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 1.7625 13.9708 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 1.7604 11.8621 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 1.7601 11.5854 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 1.7622 12.7077 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 1.7609 10.2860 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 1.7596 10.1433 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 1.7594 10.0717 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 1.7613 10.1098 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 1.7609 10.1521 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 1.7614 10.1292 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 1.7602 10.6268 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 1.7610 10.9132 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 1.7601 10.1011 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 1.7596 10.0425 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 1.7592 10.0938 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 1.7577 10.8147 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 1.7561 10.3300 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 1.7561 11.0875 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 1.7566 10.2949 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 1.7562 10.3031 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 1.7557 10.2466 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 1.7544 10.3388 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 1.7546 10.9653 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 1.7549 10.2910 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 1.7544 10.1837 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 1.7535 10.1935 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 1.7526 10.1269 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 1.7513 10.0743 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 1.7496 10.0634 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 1.7489 10.0611 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 1.7480 10.0349 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 1.7481 10.0535 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 1.7472 10.0803 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 1.7461 10.1346 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 1.7459 10.0436 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 1.7445 10.2556 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 1.7438 12.9961 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 1.7428 11.6869 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 1.7423 10.2698 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 1.7429 10.4120 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 1.7420 10.6877 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 1.7426 11.6417 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 1.7421 12.4203 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 1.7419 12.2195 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 1.7414 10.9422 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 1.7411 10.5351 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 1.7412 10.2620 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 1.7408 10.0304 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 1.7401 10.0675 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 1.7404 9.9764 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 1.7402 9.9907 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 1.7408 10.0634 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 1.7410 10.0117 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 1.7411 9.9744 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 1.7407 9.9921 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 1.7410 10.0460 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 1.7410 10.1640 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 1.7404 10.0283 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 1.7400 10.2715 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 1.7395 12.5186 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 1.7398 10.5940 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 1.7397 10.1034 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 1.7399 10.0638 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 1.7393 10.4910 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 1.7390 13.5041 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 1.7390 11.3520 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 1.7384 11.2866 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 1.7383 10.8491 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 1.7375 13.6158 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 1.7370 12.0660 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 1.7362 10.1031 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 1.7360 10.9862 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 1.7351 15.2600 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 1.7348 17.3087 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 1.7342 21.0083 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 1.7337 15.7397 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 1.7333 15.6868 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 1.7327 10.8435 sec/batch\n",
      "Validation loss: 1.5902 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 1.7328 12.5279 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 1.7327 10.5068 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 1.7322 9.8946 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 1.7318 10.3676 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 1.7311 10.7144 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 1.7306 11.7095 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 1.7301 10.8703 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 1.7297 10.4789 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 1.7294 10.9711 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 1.7287 14.4174 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 1.7281 13.7592 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 1.7274 10.7257 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 1.7270 11.1384 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 1.7267 10.1714 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 1.7261 10.2782 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 1.7257 10.4202 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 1.7251 12.6418 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 1.7247 13.9016 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 1.7243 10.6003 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 1.7240 12.2903 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 1.7237 12.4309 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 1.7234 12.2890 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 1.7231 10.7955 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 1.7227 12.8895 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 1.7223 11.2251 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 1.7219 12.5708 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 1.7213 10.9203 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 1.7206 12.1559 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 1.7203 10.5289 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 1.7199 10.0621 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 1.7195 10.0593 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 1.7190 10.1016 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.7187 10.0781 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.7180 10.0459 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.7175 11.2496 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.7173 11.1876 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.7169 14.4213 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.7162 13.3969 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.7160 13.1442 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 1.7157 11.8363 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 1.7153 10.1779 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 1.7149 10.1741 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 1.7143 10.1193 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 1.7138 10.0737 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 1.7136 10.2020 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 1.7133 10.7297 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 1.7130 13.1384 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 1.7128 14.4275 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 1.7126 12.7058 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 1.7124 13.4994 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 1.7122 13.8053 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 1.7118 12.4906 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 1.7119 12.9671 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 1.7115 4216.3423 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 1.7113 54918.8773 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 1.7111 10.6156 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 1.7108 11.3864 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 1.7106 9.7965 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 1.7104 12.2812 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 1.7104 17.0429 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 1.7102 18.9234 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 1.7098 17.9901 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 1.7094 15.6302 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 1.7092 15.1131 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 1.7090 14.4439 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 1.7088 14.1960 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 1.7085 10.2904 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 1.7083 13.4526 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 1.7081 13.5958 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 1.7078 13.4758 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 1.7074 13.2791 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 1.7073 13.3386 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 1.7073 13.4832 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 1.7070 12.7262 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 1.7068 12.4492 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 1.7066 12.8270 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 1.7063 12.4344 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 1.7060 12.3307 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 1.7058 12.6230 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 1.7060 12.5527 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 1.7056 12.0705 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 1.7054 12.0413 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 1.7050 12.5722 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 1.7047 15.3527 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 1.7045 12.7212 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 1.7043 13.1338 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 1.7040 14.3401 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 1.7036 14.8745 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 1.7032 13.6420 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 1.7031 13.5531 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 1.7500 16.2298 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 1.7033 16.1012 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 1.6838 20.4775 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 1.6780 16.0647 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 1.6713 15.4366 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 1.6607 13.6437 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 1.6606 13.9207 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 1.6581 13.6933 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 1.6590 14.3669 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 1.6568 13.9896 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 1.6523 13.2033 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 1.6506 13.4535 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 1.6502 16.5986 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 1.6522 15.7805 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 1.6511 13.9297 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 1.6495 15.5626 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 1.6492 17.0898 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 1.6508 13.9589 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 1.6507 13.3875 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 1.6511 13.4896 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 1.6508 14.9235 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 1.6515 15.5175 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 1.6501 13.2443 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 1.6495 15.9788 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 1.6494 13.4379 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 1.6481 13.4402 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 1.6468 18.8766 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 1.6472 17.4772 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 1.6482 13.2755 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 1.6480 13.4720 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 1.6478 17.1278 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 1.6466 16.5111 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 1.6469 20.5629 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 1.6471 15.7129 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 1.6469 17.8405 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 1.6463 16.4700 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 1.6454 14.4848 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 1.6440 15.0467 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 1.6424 17.2208 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 1.6418 16.0050 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 1.6409 14.6103 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 1.6410 17.8084 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 1.6401 19.8391 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 1.6391 14.4857 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 1.6393 13.2447 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 1.6380 13.5808 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 1.6375 14.6358 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 1.6369 13.6604 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 1.6363 14.9376 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 1.6369 16.5751 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 1.6360 15.7802 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 1.6366 15.7510 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 1.6361 14.6160 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 1.6359 13.5559 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 1.6355 15.5133 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 1.6355 15.9014 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 1.6356 13.4666 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 1.6351 14.5222 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 1.6344 14.6010 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 1.6348 16.0474 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 1.6346 14.4239 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 1.6354 14.3817 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 1.6357 14.7089 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 1.6357 14.4484 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 1.6354 14.4403 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 1.6354 13.6973 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 1.6355 15.4657 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 1.6350 12.9140 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 1.6347 13.2132 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 1.6344 13.1038 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 1.6347 13.1671 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 1.6347 13.1592 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 1.6349 14.0792 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 1.6344 15.0667 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 1.6341 15.0730 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 1.6342 16.1140 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 1.6337 24.4138 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 1.6335 15.6970 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 1.6328 18.2273 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.6324 20.7884 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.6318 13.8888 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.6318 14.0738 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.6311 16.2800 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.6308 15.7242 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.6302 14.9182 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.6298 16.0026 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.6293 13.5170 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.6288 13.4888 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.6281 13.3161 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.6281 13.5085 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.6276 13.9971 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.6274 13.8909 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.6269 14.5605 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.6264 14.8660 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.6259 16.2990 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.6258 18.6399 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.6254 17.4999 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.6248 16.6620 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.6243 16.9742 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.6236 16.1164 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.6235 14.3014 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.6232 14.9851 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.6229 17.1082 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.6226 14.3919 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.6222 13.3803 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.6219 13.3697 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.6216 13.4192 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.6214 15.7309 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.6212 18.3069 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.6210 17.4556 sec/batch\n",
      "Validation loss: 1.48108 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.6213 21.4749 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.6209 18.0526 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.6206 18.3626 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.6203 15.6636 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.6199 20.1669 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.6194 23.1618 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.6192 19.3488 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.6190 19.1146 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.6187 19.9686 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.6184 22.9615 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.6182 22.5692 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.6177 23.8836 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.6171 23.9160 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.6170 25.5010 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.6168 23.2408 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.6163 26.1615 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.6162 20.0847 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.6161 14.2764 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.6158 17.8743 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.6155 20.5288 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.6149 14.8617 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.6144 17.0540 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.6143 13.8759 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.6141 16.5715 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.6138 16.2866 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.6137 13.7858 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.6136 14.7900 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.6134 19.1281 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.6133 19.0926 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.6130 14.4155 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.6132 14.8959 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.6129 18.8310 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.6127 14.5439 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.6126 16.4581 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.6123 15.4092 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.6122 19.1861 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.6121 21.2359 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.6121 14.1657 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.6120 19.7864 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.6117 17.6632 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.6112 16.2866 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.6111 14.0254 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.6109 16.1787 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.6108 15.0738 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.6106 17.7084 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.6104 14.7725 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.6103 13.9220 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.6100 13.9228 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.6096 14.3071 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.6095 13.6334 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.6095 13.8464 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.6092 13.6811 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.6091 15.7835 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.6089 14.1251 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.6087 13.8499 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.6085 13.6658 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.6084 13.8035 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.6086 13.7431 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.6084 13.8863 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.6081 13.5544 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.6079 13.5452 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.6076 13.5485 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.6075 13.5644 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.6073 13.8446 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.6072 13.6633 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.6069 13.6406 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.6066 13.5217 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.6065 15.5391 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 1.6681 17.0436 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.6192 13.7148 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.6008 13.4973 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.5951 13.7961 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.5876 13.9012 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.5757 13.7526 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.5756 13.5731 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.5718 13.7045 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.5728 14.3159 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.5714 13.6704 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.5677 13.6937 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.5662 13.7954 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.5659 13.9622 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.5679 20.4930 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.5670 19.3753 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.5647 14.5923 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.5646 14.4086 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.5664 15.0001 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.5666 14.1962 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.5678 13.9134 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.5673 13.5783 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.5683 15.3838 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.5673 16.3239 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.5673 14.4598 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.5673 20.5436 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.5658 21.5390 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.5646 20.1705 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.5651 20.5940 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.5654 18.1952 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.5652 16.0896 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.5646 17.6149 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.5634 19.1375 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.5637 15.8820 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.5639 15.4852 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.5637 15.6878 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.5632 13.6858 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.5626 13.6613 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.5613 13.8352 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.5597 13.9095 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.5591 13.6627 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.5585 13.8202 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.5590 13.7649 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.5585 20.0953 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.5576 18.8150 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.5576 14.8140 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.5567 17.1245 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.5562 15.5399 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.5557 14.1420 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.5553 13.7235 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.5556 13.8910 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.5551 13.8453 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.5558 13.8899 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.5555 13.7663 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.5554 13.8168 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.5550 13.8342 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.5550 14.2549 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.5552 14.7240 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.5549 15.5902 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.5543 13.5823 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.5546 16.7282 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.5545 14.3956 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.5553 17.3967 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.5556 20.0753 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.5557 16.0883 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.5552 15.6595 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.5554 14.4672 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.5554 14.0881 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.5550 15.8918 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.5547 14.2736 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.5546 13.7893 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.5549 14.2809 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.5549 14.0224 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.5553 15.0649 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.5548 13.8894 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.5547 13.7314 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.5547 14.5961 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.5545 14.0822 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.5544 14.2292 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.5537 14.7308 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.5535 13.9102 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.5529 13.5353 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.5529 13.7806 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.5521 13.9223 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.5520 16.0253 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.5515 14.1472 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.5511 18.8517 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.5507 17.7932 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.5504 14.9493 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.5498 17.3709 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.5496 13.9395 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.5492 13.4538 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.5490 17.3916 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.5485 15.3132 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.5480 14.8619 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.5476 13.7477 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.5475 13.4987 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.5472 13.7406 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.5466 13.4877 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.5461 14.4888 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.5455 14.3552 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.5453 15.5142 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.5451 14.2489 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.5448 13.9049 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.5445 13.5721 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.5442 13.6756 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.5439 15.9458 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.5438 15.2717 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.5437 17.8549 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.5434 20.3144 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.5433 20.8866 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.5430 16.3742 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.5428 14.3457 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.5425 16.1926 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.5421 14.7361 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.5416 14.6225 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.5412 14.1015 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.5409 15.5163 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.5407 15.1534 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.5404 13.9360 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.5400 19.4895 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.5397 16.3555 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.5393 15.6340 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.5388 13.9020 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.5386 15.5272 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.5385 17.9746 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.5379 13.7885 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.5379 14.5754 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.5377 17.8131 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.5375 14.8008 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.5371 14.6893 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.5366 14.0483 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.5362 14.2013 sec/batch\n",
      "Validation loss: 1.39681 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.5367 16.6851 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.5366 14.1394 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.5364 13.7669 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.5363 14.5612 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.5363 14.7035 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.5362 14.4312 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.5361 14.0760 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.5359 17.8829 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.5361 16.5838 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.5359 17.1262 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.5358 17.8044 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.5358 20.3691 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.5355 16.4166 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.5356 16.5681 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.5355 16.3044 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.5355 16.2874 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.5355 18.9843 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.5352 23.7903 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.5348 17.8045 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.5346 19.7731 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.5345 19.2632 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.5343 16.5750 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.5341 16.3784 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.5340 17.0205 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.5339 16.5466 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.5337 16.2616 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.5334 16.4942 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.5334 16.4065 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.5335 16.1488 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.5334 16.2828 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.5332 16.3235 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.5330 17.0705 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.5329 16.2199 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.5326 16.7101 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.5326 16.4244 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.5328 16.6178 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.5327 16.7681 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.5325 17.4961 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.5323 16.4443 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.5320 16.3585 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 1.5319 16.2393 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 1.5318 16.4396 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 1.5316 16.3489 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 1.5314 16.2016 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 1.5310 16.2035 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 1.5310 16.4242 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 1.6035 16.9229 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 1.5490 16.4165 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 1.5314 16.2938 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 1.5247 16.1095 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 1.5148 16.2156 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.5054 16.2757 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.5067 16.5357 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.5034 18.6562 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 1.5035 16.7886 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 1.5019 16.1772 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 1.4972 16.5875 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 1.4954 16.1877 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 1.4945 16.5183 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 1.4962 16.3476 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 1.4949 16.4930 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 1.4932 16.2848 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 1.4936 16.1590 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 1.4951 16.5064 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 1.4950 16.4445 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 1.4963 17.2758 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 1.4958 16.2332 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 1.4960 16.5732 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 1.4951 16.2148 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 1.4942 18.2581 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 1.4939 16.2697 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 1.4924 18.9543 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 1.4911 18.5562 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 1.4917 16.8758 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 1.4922 16.3472 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 1.4924 18.6675 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 1.4914 17.8134 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 1.4905 17.4985 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 1.4906 21.0629 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 1.4908 21.1751 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 1.4903 24.2304 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 1.4897 19.3750 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 1.4888 15.8886 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 1.4876 21.0616 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 1.4858 21.4128 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 1.4851 15.5083 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 1.4845 14.7372 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 1.4850 21.4043 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 1.4845 18.4842 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 1.4836 18.3730 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 1.4841 18.4602 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 1.4830 19.4884 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 1.4827 20.3952 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 1.4820 21.0930 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 1.4818 19.2961 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 1.4821 18.7800 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 1.4817 18.7033 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 1.4823 16.8105 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 1.4821 24.0593 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 1.4823 15.8731 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 1.4819 19.4624 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 1.4818 18.1986 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 1.4821 15.5919 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 1.4818 14.5270 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 1.4813 13.9266 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 1.4818 14.9191 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 1.4815 18.2619 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 1.4823 15.8510 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 1.4825 14.8294 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 1.4826 15.0604 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 1.4824 19.0844 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 1.4824 21.6741 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 1.4825 16.7854 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 1.4820 20.2843 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 1.4819 16.8634 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 1.4819 21.2384 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 1.4823 24.4505 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 1.4824 17.0437 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 1.4827 17.1492 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 1.4823 16.3891 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 1.4821 14.6899 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 1.4823 15.7884 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 1.4819 16.0342 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 1.4818 14.3943 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 1.4810 15.8411 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 1.4808 18.0132 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 1.4802 22.9906 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 1.4801 19.9197 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 1.4795 18.5686 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 1.4793 16.4186 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 1.4788 19.9859 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 1.4784 16.5178 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 1.4781 16.2554 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 1.4777 18.1722 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 1.4772 16.1219 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 1.4773 20.6809 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 1.4769 19.4139 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 1.4766 14.4269 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 1.4761 4201.9782 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 1.4757 10.6476 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.4753 9041.2223 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.4751 10.1002 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.4749 11.2966 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.4745 12.3176 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.4738 16.8376 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.4732 14.9072 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.4730 17.5695 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.4728 13.9227 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.4725 15.2536 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.4722 18.8387 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.4719 16.8016 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.4716 15.7419 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.4713 15.8707 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.4711 13.2592 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.4709 15.4117 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.4708 14.3019 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.4705 15.7159 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.4702 15.2805 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.4699 15.7828 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.4695 14.2504 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.4691 13.6173 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.4686 13.7659 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.4685 13.6260 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.4685 13.6412 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.4684 12.3631 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.4683 14.7795 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.4682 13.1205 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.4677 13.2764 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.4673 14.7666 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.4671 16.1256 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.4670 13.3307 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.4665 14.0260 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.4664 12.0297 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.4664 16.3463 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.4661 15.7035 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.4658 18.0260 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.4653 14.4633 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.4650 15.5245 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.4650 13.7949 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.4649 13.6746 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.4648 14.4406 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.4647 13.4994 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.4648 14.4954 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.4648 14.0224 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.4647 14.4147 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.4645 17.6510 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.4649 15.8639 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.4648 16.8256 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.4646 15.7420 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.4649 14.2910 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.4647 14.4526 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.4647 13.2921 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.4647 14.5140 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.4648 16.3236 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.4648 16.7864 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.4646 15.1196 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.4642 16.9112 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.4640 14.0416 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.4639 13.5490 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.4639 15.8824 sec/batch\n",
      "Validation loss: 1.33161 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.4644 15.8195 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.4644 13.6434 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.4644 13.8400 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.4643 13.6220 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.4640 13.5703 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.4640 14.9468 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.4641 14.2164 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.4639 14.1901 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.4637 14.1278 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.4637 12.9519 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.4636 13.4644 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.4634 15.2457 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.4635 15.1605 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.4638 14.6572 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.4637 13.4035 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.4635 15.6209 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.4633 13.6301 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.4630 13.4656 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.4630 13.1237 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.4629 19.3132 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.4629 14.2677 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.4626 15.1437 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.4624 13.2001 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.4623 13.4069 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 1.5570 14.1264 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 1.5070 15.2940 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.4865 12.9452 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.4790 12.8443 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.4668 13.0038 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.4546 12.8031 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.4547 12.7993 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.4516 12.9729 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.4505 12.8142 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.4491 12.7914 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.4453 12.8899 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.4440 12.8224 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.4433 12.7845 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.4446 13.0180 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.4428 12.9083 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.4407 16.6750 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.4403 17.8883 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.4416 14.9345 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.4411 12.9047 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.4420 13.2206 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.4408 14.0943 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.4412 13.7760 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.4402 15.5817 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.4397 14.0878 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.4393 16.4157 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.4373 14.3619 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.4356 14.4577 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.4357 14.5642 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.4358 14.9231 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.4359 18.3605 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.4356 15.7082 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.4349 14.8240 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.4350 14.5333 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.4351 14.1704 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.4348 14.3116 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.4346 14.3007 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.4338 14.2716 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.4325 14.1418 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.4313 14.2550 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.4306 14.1902 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.4298 14.0061 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.4302 14.6062 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.4297 15.1572 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.4288 16.4737 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.4288 15.9241 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.4279 14.0688 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.4274 14.0954 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.4269 15.3587 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.4268 17.8359 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.4269 15.3823 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.4264 14.8633 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.4271 13.6244 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.4268 13.5148 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.4269 15.5112 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.4264 14.7807 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.4265 15.5793 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.4267 14.4634 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.4263 13.5482 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.4258 14.5510 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.4261 15.3637 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.4260 14.1688 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.4268 14.9401 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.4270 17.7013 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.4270 13.3366 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.4268 13.6142 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.4269 14.6215 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.4270 15.0558 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.4265 14.5361 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.4265 14.3030 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.4262 13.4282 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.4267 14.7516 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.4269 17.8194 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.4273 14.9902 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.4270 13.4800 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.4268 13.4713 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.4269 14.5196 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.4267 15.9573 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.4265 16.2920 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.4258 13.9098 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.4258 14.0836 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.4251 15.5190 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.4250 15.5624 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.4243 13.3950 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.4241 15.0470 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.4238 17.8000 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.4234 13.3002 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.4230 14.6473 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.4227 15.2606 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.4223 13.9822 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.4224 15.6123 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.4221 15.6882 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.4218 15.2491 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.4213 13.6592 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.4209 15.0731 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.4205 17.3799 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.4205 14.2178 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.4203 13.6978 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.4198 13.3900 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.4193 13.8779 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.4189 14.0438 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.4188 17.2792 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.4187 15.1032 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.4184 14.0139 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.4182 14.9234 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.4180 18.9550 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.4178 16.9820 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.4177 19.0279 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.4176 14.1575 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.4174 13.5975 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.4174 14.8602 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.4171 14.9400 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.4169 15.2995 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.4167 14.0890 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.4165 14.1284 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.4160 13.9704 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.4156 13.8376 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.4155 13.2925 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.4154 13.1628 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.4152 13.1925 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.4151 13.3201 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.4149 14.0794 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.4145 21.2715 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.4139 15.0622 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.4139 13.1380 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.4137 13.2830 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.4133 14.0007 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.4133 14.5760 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.4132 15.4979 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.4130 13.8240 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.4126 13.2431 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.4121 13.5895 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.4118 13.9078 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.4118 15.9520 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.4119 13.7412 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.4117 13.9694 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.4117 13.1043 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.4118 13.6561 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.4119 14.7073 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.4118 17.8243 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.4118 16.4015 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.4121 13.8153 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.4120 13.4366 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.4119 14.5369 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.4121 20.4384 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.4119 19.2622 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.4120 14.8472 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.4120 13.6595 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.4121 13.4639 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.4122 13.4802 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.4120 15.2806 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.4117 15.3876 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.4114 14.2218 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.4114 14.5753 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.4114 15.8931 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.4113 14.9986 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.4112 14.1199 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.4112 17.8156 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.4111 19.0263 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.4107 21.2082 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.4108 22.1686 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.4109 20.6940 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.4107 19.6819 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.4107 19.9757 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.4106 20.4508 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.4106 19.3789 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.4104 19.3554 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.4105 19.7434 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.4109 19.8563 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.4109 19.4189 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.4107 20.1353 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.4106 19.4602 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.4104 19.8526 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.4105 19.2130 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.4104 19.3356 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.4104 19.4251 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.4102 19.4995 sec/batch\n",
      "Validation loss: 1.28431 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.4108 14.1299 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.4110 13.6193 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 1.5162 13.5843 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.4658 13.6189 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.4399 13.4748 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.4340 14.0393 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.4227 13.6817 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.4108 13.7953 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.4120 13.6958 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.4091 13.7234 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.4093 13.8252 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.4088 17.3195 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.4049 13.4229 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.4034 13.4840 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.4030 13.3956 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.4042 13.4052 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.4031 13.5158 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.4008 13.2879 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.4009 13.3595 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.4021 13.4034 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.4016 13.5072 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.4023 13.3010 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.4014 13.3799 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.4013 13.5015 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.4000 13.4096 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.3995 13.3996 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.3988 13.4818 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.3969 13.3337 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.3956 13.4440 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.3963 13.3460 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.3964 13.4525 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.3963 13.4380 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.3959 13.3789 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.3946 13.4877 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.3950 13.3816 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.3949 13.8180 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.3943 13.3292 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.3937 13.4668 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.3926 13.4694 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.3911 13.4661 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.3896 13.6600 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.3890 13.4503 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.3885 13.3908 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.3892 13.4564 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.3886 13.3994 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.3879 13.4933 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.3880 13.4820 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.3869 16.8027 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.3867 15.0802 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.3861 19.6364 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.3859 20.7331 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.3862 25.5497 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.3856 27.4497 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.3863 28.8428 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.3860 29.8366 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.3862 24.0877 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.3858 21.0609 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.3858 17.6948 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.3860 20.5397 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.3855 21.8969 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.3849 20.6970 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.3854 28.6544 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.3852 25.3959 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.3860 24.1510 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.3863 24.4200 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.3863 25.2616 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.3860 19.7106 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.3861 22.8628 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.3864 20.2631 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.3859 19.5565 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.3861 19.5469 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.3859 19.8130 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.3861 19.1180 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.3863 19.3293 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.3867 19.2906 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.3863 19.2208 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.3862 19.1492 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.3864 19.1752 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.3862 19.1705 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.3860 19.1927 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.3852 19.5404 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.3851 19.0611 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.3844 19.1328 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.3843 18.9456 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.3837 18.9342 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.3834 18.9378 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.3830 19.0893 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.3827 19.6867 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.3824 20.4018 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.3820 14.4685 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.3817 13.2205 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.3818 12.9724 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.3815 13.1369 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.3813 13.0925 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.3809 13.0678 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.3804 12.9968 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.3800 12.8187 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.3799 13.0253 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.3799 12.9200 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.3794 12.8598 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.3791 12.9396 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.3786 12.9890 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.3786 13.0384 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.3784 12.9590 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.3782 12.8472 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.3780 13.1919 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.3779 12.9781 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.3777 13.0204 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.3775 13.0138 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.3775 12.8654 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.3773 12.9672 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.3773 12.9583 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.3770 12.9805 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.3769 13.2207 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.3767 12.9970 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.3765 14.2469 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.3761 14.0930 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.3757 4216.7253 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.3755 39419.5396 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.3755 11.8615 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.3753 14.1463 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.3751 14.0631 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.3750 14.0507 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.3745 10.8092 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.3741 10.0883 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.3740 10.3506 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.3738 10.5172 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.3734 9.9556 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.3734 10.2545 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.3734 11.2490 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.3732 11.1534 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.3729 12.0820 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.3725 12.1204 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.3722 11.1379 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.3722 10.4445 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.3722 10.5648 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.3721 10.2361 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.3720 10.2563 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.3722 10.0580 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.3722 11.3774 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.3721 12.3390 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.3721 11.0739 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.3724 11.1235 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.3723 11.2714 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.3722 11.9897 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.3723 11.4246 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.3722 11.2457 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.3723 10.9447 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.3723 10.8089 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.3725 10.7735 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.3725 10.2141 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.3724 12.8034 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.3720 11.1276 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.3719 11.0291 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.3719 10.7092 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.3718 10.1896 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.3718 10.0974 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.3717 9.7327 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.3717 10.3621 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.3715 9.7599 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.3713 10.2917 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.3713 10.7556 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.3714 10.7834 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.3713 10.7716 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.3713 10.5721 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.3712 9.9206 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.3711 10.3773 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.3711 10.7737 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.3712 10.4117 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.3716 10.1654 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.3715 2307.5952 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.3714 11.3901 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.3713 9.7120 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.3711 9.7792 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.3711 9.6078 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.3711 9.7636 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.3711 9.7066 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.3709 11.7536 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.3707 13.2131 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.3708 9.6750 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.4837 10.1891 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.4428 10.0366 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.4295 10.0459 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.4187 10.4229 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.4049 10.8059 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.3924 10.5472 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.3904 10.7625 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.3862 10.7903 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.3848 15.7685 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.3823 12.7165 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.3775 15.4111 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.3753 13.0754 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.3735 10.6415 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.3749 11.1744 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.3733 11.1476 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.3710 10.5400 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.3709 14.9726 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.3712 11.7253 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.3705 16.9570 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.3712 11.7076 sec/batch\n",
      "Validation loss: 1.24982 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.3773 13.1744 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.3777 10.1413 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.3763 10.3834 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.3756 9.8366 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.3750 9.9575 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.3730 10.2916 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.3714 11.4179 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.3716 10.8698 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.3714 11.4445 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.3714 11.7579 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.3704 11.6404 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.3689 11.5097 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.3685 10.0071 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.3685 10.8471 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.3678 10.0398 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.3671 10.8858 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.3658 12.5890 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.3642 11.2881 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.3625 10.2949 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.3615 9.8206 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.3610 9.8315 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.3614 9.8877 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.3609 9.7683 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.3599 9.7558 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.3595 9.7533 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.3584 9.8490 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.3579 9.8657 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.3573 9.7844 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.3571 9.8512 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.3573 11.6722 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.3566 11.5577 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.3574 11.4034 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.3569 9.7654 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.3569 9.7809 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.3565 10.1648 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.3563 10.7874 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.3566 13.2372 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.3561 10.8046 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.3554 11.5214 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.3559 10.2132 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.3556 10.1436 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.3564 12.0326 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.3566 12.1323 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.3567 10.9597 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.3564 9.7755 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.3563 9.9221 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.3564 9.9058 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.3559 9.6588 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.3559 10.7764 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.3556 10.7698 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.3560 10.8304 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.3562 11.3601 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.3565 9.6356 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.3561 9.7455 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.3559 9.7149 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.3559 9.7170 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.3557 9.7065 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.3555 9.9207 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.3548 9.8531 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.3547 10.2060 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.3542 9.6878 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.3540 9.7820 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.3535 9.6210 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.3534 9.8464 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.3530 9.7886 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.3526 10.1756 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.3523 10.6583 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.3519 9.8649 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.3514 9.6802 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.3514 9.8707 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.3511 10.9749 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.3509 10.4650 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.3505 9.7947 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.3501 9.8160 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.3499 9.9036 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.3499 10.0080 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.3497 9.8967 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.3493 9.7512 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.3488 9.7877 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.3485 9.7612 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.3484 9.7321 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.3482 10.1494 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.3480 9.7686 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.3478 9.9072 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.3474 9.7261 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.3472 9.7023 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.3471 9.9165 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.3470 12.6642 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.3468 11.6495 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.3467 11.4237 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.3464 12.3726 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.3462 10.7175 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.3461 10.1657 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.3458 9.6681 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.3455 9.7157 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.3451 9.6345 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.3450 9.6852 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.3449 9.6802 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.3447 10.0512 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.3446 9.6647 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.3445 9.8045 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.3440 10.2303 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.3435 10.5500 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.3434 11.6372 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.3432 9.7650 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.3428 10.7930 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.3428 9.7359 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.3427 9.9100 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.3426 11.1523 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.3422 10.6661 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.3417 11.2397 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.3414 10.2542 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.3415 9.6727 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.3414 9.6741 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.3413 9.6749 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.3413 9.6621 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.3414 9.6417 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.3415 10.1964 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.3414 9.6672 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.3414 9.6924 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.3418 11.9310 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.3418 11.4533 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.3417 10.1649 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.3419 11.5743 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.3417 9.8248 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.3418 10.4773 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.3418 9.7909 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.3421 11.1117 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.3421 9.9036 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.3420 11.0440 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.3416 11.6073 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.3414 9.8136 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.3414 9.6368 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.3414 9.7316 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.3414 9.6896 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.3413 9.6550 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.3413 9.6682 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.3412 9.6871 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.3409 9.9149 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.3410 10.0621 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.3411 13.8452 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.3409 10.4410 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.3408 9.8867 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.3408 11.6834 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.3408 10.4758 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.3407 12.6216 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.3408 9.9703 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.3412 10.6325 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.3412 10.5101 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.3412 10.2177 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.3410 11.3551 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.3408 10.1541 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.3409 12.0436 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.3409 10.8991 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.3408 10.5338 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.3407 10.5600 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.3405 11.2019 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.3406 10.1682 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.4471 11.2355 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.4018 10.0070 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.3775 9.7251 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.3706 9.7719 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.3608 9.7447 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.3495 9.8127 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.3498 9.8063 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.3456 9.8045 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.3438 9.7481 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.3420 9.7897 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.3378 9.7262 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.3364 9.7208 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.3356 9.7330 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.3362 9.7382 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.3348 9.6883 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.3331 9.9064 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.3328 11.8020 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.3344 12.2647 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.3343 10.1552 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.3349 10.8853 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.3340 9.8598 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.3341 9.8438 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.3331 11.0075 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.3329 12.1535 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.3325 9.8540 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.3309 9.7317 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.3293 9.7312 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.3298 9.9581 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.3298 11.3716 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.3299 12.6846 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.3291 10.1533 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.3280 9.9011 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.3282 9.8434 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.3285 10.0124 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.3283 10.2587 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.3278 10.3242 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.3268 10.7302 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.3255 11.1173 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.3241 9.7962 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.3234 9.9060 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.3227 9.8707 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.3232 9.8260 sec/batch\n",
      "Validation loss: 1.21943 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.3270 11.0588 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.3268 9.7730 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.3271 9.7645 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.3264 9.7969 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.3262 9.6718 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.3259 9.6853 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.3260 9.7169 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.3263 9.7030 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.3259 9.6895 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.3267 9.7689 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.3265 9.9849 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.3266 9.8072 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.3262 10.8412 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.3264 10.2031 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.3268 11.1291 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.3265 10.0154 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.3260 10.0211 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.3265 9.8284 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.3263 9.8336 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.3271 9.8622 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.3273 9.7939 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.3274 9.7979 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.3271 9.7415 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.3272 9.8699 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.3272 9.8550 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.3269 9.7396 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.3270 9.9106 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.3268 9.9488 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.3272 9.9037 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.3274 9.9460 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.3278 9.9415 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.3273 11.5830 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.3272 16.2738 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.3272 12.3038 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.3271 10.8890 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.3269 10.5394 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.3262 11.5215 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.3262 10.4722 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.3256 10.5654 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.3256 12.8179 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.3250 14.4413 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.3249 12.6359 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.3245 12.8052 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.3243 10.3348 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.3240 9.9382 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.3237 10.0975 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.3232 10.1412 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.3233 11.5581 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.3231 14.2652 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.3230 10.9966 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.3225 12.2958 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.3221 11.9078 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.3218 12.3204 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.3217 10.8149 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.3216 12.1909 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.3211 12.1502 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.3207 10.7750 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.3203 11.1474 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.3203 10.1144 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.3201 10.0149 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.3198 10.2298 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.3196 10.4530 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.3194 10.0398 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.3193 9.9856 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.3192 9.9007 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.3192 9.9433 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.3190 9.9120 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.3189 9.9380 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.3187 10.0297 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.3186 11.9389 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.3184 11.5235 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.3182 10.9223 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.3178 11.4751 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.3175 10.0490 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.3174 10.3830 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.3174 10.0628 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.3172 9.9707 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.3170 9.9773 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.3168 9.9642 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.3164 9.9683 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.3159 9.9167 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.3159 9.9637 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.3158 10.0011 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.3154 9.9876 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.3155 10.0247 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.3154 9.9834 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.3152 9.9998 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.3148 10.2377 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.3144 9.9423 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.3142 11.2399 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.3142 9.9853 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.3142 9.9768 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.3141 9.9738 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.3141 9.9922 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.3142 10.0333 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.3143 9.9688 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.3142 9.9978 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.3142 10.0401 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.3145 9.9659 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.3145 9.8906 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.3144 10.0626 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.3146 10.0292 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.3145 11.6408 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.3146 10.6319 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.3146 11.8184 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.3149 11.2561 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.3149 13.3356 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.3148 10.9172 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.3144 12.6438 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.3142 11.4555 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.3143 10.8941 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.3142 10.0399 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.3142 10.0715 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.3141 11.7430 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.3141 13.0477 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.3140 11.0399 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.3137 11.9539 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.3138 10.7756 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.3139 12.9203 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.3139 10.2498 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.3138 11.9952 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.3137 11.3776 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.3138 10.0120 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.3137 10.5936 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.3138 10.1575 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.3142 10.6494 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.3142 10.6500 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.3141 10.6644 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.3140 10.8101 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.3138 9.9478 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.3139 9.8470 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.3139 10.0619 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.3139 9.9430 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.3138 9.9182 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.3136 9.8656 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.3138 9.9007 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.4181 9.8803 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.3714 9.8659 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.3516 9.8426 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.3475 9.9028 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.3374 10.0104 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.3256 10.0567 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.3245 9.9789 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.3204 10.4835 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.3182 10.3585 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.3165 10.1046 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.3125 10.4919 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.3116 12.7628 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.3115 10.0573 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.3116 9.9665 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.3101 10.0147 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.3082 9.9439 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.3083 9.9089 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.3096 9.9270 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.3097 9.8776 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.3109 9.8570 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.3100 9.8845 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.3104 9.9167 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.3094 9.8748 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.3093 9.9219 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.3090 9.7981 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.3073 9.8255 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.3062 9.9575 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.3069 9.8350 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.3070 9.8736 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.3070 9.8641 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.3063 9.8832 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.3050 10.1536 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.3050 10.1497 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.3050 10.2827 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.3049 9.9181 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.3046 10.4550 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.3039 16.3098 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.3025 12.9332 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.3012 10.0797 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.3004 9.9065 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.2999 9.8904 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.3006 9.8371 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.3004 9.9451 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.2997 9.8412 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.2999 10.1556 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.2991 11.9195 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.2988 10.8342 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.2985 11.0726 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.2985 10.6605 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.2989 10.5837 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.2984 10.0715 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.2992 10.2831 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.2991 11.0560 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.2993 10.2670 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.2990 9.9283 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.2989 9.9305 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.2990 9.9206 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.2987 9.9795 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.2983 10.7151 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.2988 10.2736 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.2989 10.0519 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.2994 10.4745 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.2997 9.9062 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.2998 9.8741 sec/batch\n",
      "Validation loss: 1.20213 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.3023 11.2369 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.3026 10.1832 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.3029 9.8345 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.3026 10.0898 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.3026 9.7174 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.3024 9.9843 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.3030 10.1309 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.3033 10.7916 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.3038 10.8082 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.3035 9.9520 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.3034 9.9789 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.3036 10.0018 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.3035 10.0568 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.3033 10.0077 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.3027 10.7749 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.3027 12.5178 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.3022 10.6972 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.3022 10.3065 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.3016 11.9962 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.3016 10.5888 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.3012 10.0155 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.3010 9.9268 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.3007 10.0742 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.3005 9.9785 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.3001 9.9862 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.3001 10.1047 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.3000 10.4741 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.2999 10.0917 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.2994 10.0386 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.2990 10.5402 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.2986 11.5765 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.2986 11.7506 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.2986 10.9819 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.2980 11.7913 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.2977 10.2256 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.2974 10.3560 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.2974 11.1052 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.2972 11.7270 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.2971 10.4396 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.2970 9.9894 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.2969 9.8749 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.2967 11.9696 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.2966 10.4368 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.2966 10.0626 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.2964 9.9790 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.2965 9.9733 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.2962 9.9041 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.2961 11.3600 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.2959 10.8284 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.2958 10.3442 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.2954 11.0410 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.2950 11.8377 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.2949 10.5519 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.2949 10.0384 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.2948 9.9737 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.2947 10.4576 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.2945 13.1509 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.2940 10.5464 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.2936 10.0284 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.2935 10.0620 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.2934 10.0394 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.2930 9.9751 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.2930 10.0317 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.2930 9.9982 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.2928 11.9569 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.2924 15.2205 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.2920 14.9688 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.2918 13.8141 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.2918 12.8079 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.2918 12.5394 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.2917 12.5628 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.2917 11.1266 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.2919 11.5293 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.2919 11.2652 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.2918 10.3469 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.2918 10.7434 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.2921 11.2479 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.2921 10.9666 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.2920 13.0625 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.2921 11.4290 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.2920 12.6343 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.2921 12.1659 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.2922 11.0583 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.2924 12.2005 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.2925 12.2381 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.2924 10.6790 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.2921 10.1690 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.2919 10.1592 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.2919 10.0358 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.2918 9.8984 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.2917 9.8864 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.2916 9.8668 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.2916 9.9657 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.2915 10.5993 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.2912 11.3104 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.2913 11.7307 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.2915 10.1061 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.2914 10.1124 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.2914 10.0345 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.2913 10.6198 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.2913 10.1425 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.2913 10.0262 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.2914 9.9565 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.2918 10.0004 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.2917 9.9214 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.2917 10.0460 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.2916 9.9370 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.2914 9.9421 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.2915 11.0318 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.2915 10.0479 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.2915 10.4556 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.2914 12.2376 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.2913 10.0209 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.2914 9.9246 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.3947 9.9716 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.3493 11.1334 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.3286 9.9948 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.3230 10.0593 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.3118 10.2998 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.2984 10.5879 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.2983 10.8522 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.2947 12.9835 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.2942 11.7412 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.2931 10.2743 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.2897 10.1254 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.2890 10.3619 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.2891 11.4997 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.2906 11.1849 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.2893 11.2019 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.2870 10.1269 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.2878 10.0590 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.2892 10.5938 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.2888 10.1203 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.2897 10.0329 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.2890 10.0604 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.2892 10.0165 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.2884 12.0230 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.2887 10.3004 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.2884 10.3387 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.2869 10.2140 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.2857 9.9540 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.2861 9.9427 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.2861 9.9597 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.2863 9.9777 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.2857 9.9477 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.2845 10.3988 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.2847 9.9744 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.2847 10.0636 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.2843 9.8787 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.2839 9.9481 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.2830 9.9709 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.2816 9.9650 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.2803 9.9885 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.2799 10.0358 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.2794 9.9320 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.2802 10.7237 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.2799 9.9142 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.2793 9.8703 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.2794 9.9761 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.2786 10.0913 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.2782 9.9063 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.2778 9.8811 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.2776 9.8610 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.2778 10.0142 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.2773 9.9723 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.2778 9.9229 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.2778 9.9008 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.2782 9.9453 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.2779 9.8897 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.2779 9.9472 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.2783 11.6750 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.2781 12.4551 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.2777 10.2318 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.2785 10.1285 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.2784 9.9543 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.2791 10.8695 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.2793 10.0297 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.2793 10.0853 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.2791 9.9801 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.2793 9.9146 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.2795 10.1476 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.2791 10.8562 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.2792 10.1891 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.2790 9.9280 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.2795 9.8743 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.2799 9.8540 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.2802 9.9106 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.2798 9.8582 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.2798 9.8817 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.2799 11.0630 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.2798 9.9196 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.2796 9.9426 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.2790 9.9761 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.2789 9.9633 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.2784 10.0051 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.2782 9.9616 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.2777 11.4398 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.2776 10.1449 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.2773 10.0783 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.2771 10.0077 sec/batch\n",
      "Validation loss: 1.18583 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.2790 11.4091 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.2788 10.0543 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.2785 13.0300 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.2786 11.9825 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.2784 10.7705 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.2783 11.9103 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.2779 10.3447 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.2776 10.0634 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.2775 12.4303 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.2775 12.6246 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.2775 11.7411 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.2771 12.8346 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.2768 11.9906 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.2764 12.7006 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.2764 11.4018 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.2764 14.5256 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.2762 11.0764 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.2761 10.3785 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.2759 11.3010 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.2757 10.3289 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.2756 12.9659 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.2756 11.2054 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.2754 13.4785 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.2754 10.7634 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.2752 10.1676 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.2750 10.4065 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.2749 12.0049 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.2747 12.6279 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.2743 12.3295 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.2740 10.4629 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.2739 10.1632 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.2739 10.1602 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.2738 10.1659 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.2737 12.1130 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.2736 12.2798 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.2732 13.2567 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.2729 10.5943 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.2728 11.0247 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.2727 11.2208 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.2724 10.4942 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.2724 10.6284 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.2724 10.7647 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.2722 10.4474 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.2720 11.7136 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.2716 10.0868 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.2713 10.8630 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.2714 11.7660 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.2714 10.4332 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.2714 10.3909 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.2714 10.3589 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.2716 14.8037 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.2716 11.3882 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.2716 15.5263 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.2716 13.0042 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.2719 12.8367 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.2720 11.5724 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.2718 12.4228 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.2720 10.9732 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.2719 10.5688 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.2721 11.2187 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.2720 14.4464 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.2722 12.1376 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.2723 12.0419 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.2722 11.3885 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.2719 10.4028 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.2717 10.5020 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.2717 10.2300 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.2717 12.8065 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.2716 12.5089 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.2716 12.0912 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.2716 13.5607 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.2714 12.0546 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.2712 11.2607 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.2713 11.6565 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.2714 13.0887 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.2714 14.0746 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.2713 12.4542 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.2712 12.5291 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.2712 11.0512 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.2712 10.1785 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.2713 10.2541 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.2716 10.1210 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.2717 11.1911 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.2716 10.2294 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.2715 10.4155 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.2714 10.6020 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.2715 10.1846 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.2715 10.4436 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.2715 10.9841 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.2713 13.9755 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.2712 10.9291 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.2714 10.4042 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.3793 11.4946 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.3322 11.6978 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.3158 10.1133 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.3066 10.0282 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.2942 9.9254 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.2835 10.1778 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.2832 9.9305 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.2789 9.9734 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.2783 10.1865 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.2774 10.0750 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.2742 10.2263 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.2725 12.5324 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.2725 15.9142 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.2731 12.4960 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.2723 11.7312 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.2704 12.2668 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.2704 11.4191 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.2713 14.1000 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.2706 12.8521 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.2713 12.1454 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.2704 11.2441 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.2706 11.4111 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.2696 10.1850 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.2699 13.6025 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.2700 12.8130 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.2686 11.6346 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.2674 10.3998 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.2681 10.2334 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.2684 10.9167 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.2683 10.2063 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.2677 10.3221 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.2664 11.3049 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.2663 10.3035 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.2664 10.1910 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.2660 10.2121 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.2657 10.1839 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.2649 10.1631 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.2636 10.1792 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.2623 10.6599 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.2618 10.1399 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.2612 10.0082 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.2620 10.1198 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.2617 9.9758 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.2611 10.0781 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.2613 13.3955 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.2606 10.9074 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.2602 11.5505 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.2599 10.1301 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.2598 10.1364 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.2600 10.1328 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.2596 11.3491 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.2601 10.0999 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.2600 10.1260 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.2602 10.1050 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.2601 11.6453 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.2599 10.6594 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.2603 11.6836 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.2600 12.2102 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.2593 11.9952 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.2599 11.0721 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.2598 10.5388 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.2605 10.0859 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.2606 12.0194 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.2606 12.2193 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.2604 10.1741 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.2604 10.0749 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.2606 11.0575 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.2602 10.5489 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.2604 9.9759 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.2602 9.9800 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.2609 12.7258 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.2611 10.6632 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.2615 10.6690 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.2612 12.5277 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.2611 10.2186 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.2612 10.0229 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.2611 10.1720 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.2611 10.0117 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.2604 9.9243 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.2604 10.0181 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.2598 10.8167 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.2597 10.0083 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.2594 9.9326 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.2592 9.9643 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.2589 10.0020 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.2588 10.7155 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.2585 10.5932 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.2582 13.3495 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.2579 10.5659 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.2579 10.7041 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.2577 11.3660 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.2576 11.4637 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.2571 10.5622 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.2568 10.3570 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.2567 10.0248 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.2567 10.0093 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.2567 10.0026 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.2563 9.9884 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.2559 10.2253 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.2556 10.0319 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.2556 9.9433 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.2554 9.9089 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.2553 10.2660 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.2551 10.0307 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.2551 10.0712 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.2549 10.3254 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.2550 10.0463 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.2549 10.6895 sec/batch\n",
      "Validation loss: 1.17012 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.2565 11.6104 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.2567 10.0164 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.2566 9.8131 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.2566 9.7741 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.2566 9.8772 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.2565 9.8721 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.2562 10.0312 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.2559 9.9957 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.2559 11.4824 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.2559 10.8557 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.2558 11.4244 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.2557 10.5256 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.2555 9.9611 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.2552 10.3995 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.2547 12.4643 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.2547 13.2268 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.2546 11.3245 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.2542 14.7981 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.2542 10.5482 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.2542 11.8765 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.2540 13.4493 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.2536 14.6222 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.2532 14.3199 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.2530 13.9831 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.2532 14.9791 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.2532 14.6568 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.2531 11.2842 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.2532 12.2902 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.2533 12.4919 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.2534 12.2072 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.2534 10.6278 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.2534 10.2076 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.2537 10.1563 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.2537 10.1993 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.2536 10.1421 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.2538 10.3967 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.2537 14.3666 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.2539 12.3365 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.2539 13.7066 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.2541 10.7778 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.2541 10.1781 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.2540 10.6185 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.2537 10.0610 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.2535 9.9659 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.2536 11.6558 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.2536 13.1607 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.2536 10.2678 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.2536 10.3408 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.2535 11.0077 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.2535 10.5093 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.2532 11.9894 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.2533 10.9363 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.2535 12.8019 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.2535 10.6346 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.2534 10.9019 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.2535 10.1130 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.2535 9.9734 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.2534 10.0709 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.2535 10.3197 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.2539 15.5168 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.2539 16.4308 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.2539 10.9133 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.2538 10.0547 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.2537 10.0776 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.2538 10.7357 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.2538 10.3888 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.2538 10.5033 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.2537 11.5864 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.2535 10.6473 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.2538 9.9555 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.3548 10.0711 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.3123 10.0151 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.2975 10.0106 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.2903 10.3993 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.2788 9.9555 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.2683 9.9703 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.2681 9.9630 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.2652 9.9479 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.2637 9.9491 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.2627 9.8902 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.2587 10.0204 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.2576 9.9700 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.2575 10.1201 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.2581 9.9809 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.2571 10.5365 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.2548 10.1063 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.2546 9.8790 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.2558 9.9948 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.2552 9.8923 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.2561 9.8900 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.2556 9.8885 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.2558 9.9621 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.2548 9.9731 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.2549 9.8857 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.2548 9.8303 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.2534 9.9568 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.2522 9.9506 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.2527 9.8938 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.2526 9.8614 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.2530 9.8830 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.2521 9.8538 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.2508 9.8671 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.2509 9.8508 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.2509 9.9686 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.2507 9.9277 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.2502 10.0352 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.2495 9.9484 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.2481 9.9612 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.2468 9.9166 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.2465 9.9232 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.2456 9.8495 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.2463 9.7925 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.2459 9.9495 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.2453 10.2247 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.2454 10.6752 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.2447 10.2045 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.2441 10.2348 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.2437 10.1150 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.2436 10.6034 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.2440 10.8088 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.2435 10.2537 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.2442 9.9255 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.2441 10.1057 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.2441 10.3955 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.2439 10.0142 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.2441 9.9086 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.2443 9.9227 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.2441 9.9081 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.2436 9.9400 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.2443 9.9194 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.2441 9.9074 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.2448 9.9920 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.2451 10.0758 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.2452 10.0529 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.2450 10.0043 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.2450 10.0814 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.2452 10.0545 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.2449 10.1442 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.2450 10.0152 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.2449 9.9713 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.2455 10.0052 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.2458 10.1125 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.2462 10.0961 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.2458 10.1733 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.2457 13.8616 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.2457 10.4401 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.2456 10.3108 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.2456 10.0955 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.2449 11.0005 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.2448 10.2452 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.2444 10.1617 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.2444 10.0508 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.2439 10.0700 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.2438 11.8008 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.2435 10.0536 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.2434 9.9368 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.2432 10.1984 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.2430 11.1994 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.2427 11.4478 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.2429 10.1018 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.2426 10.0647 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.2425 10.1041 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.2421 10.1480 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.2418 11.7634 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.2416 10.9564 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.2418 10.3596 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.2417 9.9164 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.2413 10.2975 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.2409 10.7776 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.2406 10.6858 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.2406 10.4165 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.2404 10.0008 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.2404 11.1661 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.2402 10.9718 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.2401 11.1703 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.2399 11.2210 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.2397 11.2453 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.2396 11.1944 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.2395 14.4901 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.2395 12.0970 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.2394 11.5640 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.2393 11.8536 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.2392 13.1869 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.2391 11.7686 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.2388 12.5967 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.2385 12.4067 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.2385 11.1156 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.2385 10.4874 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.2383 10.0969 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.2382 9.9368 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.2381 10.1078 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.2378 11.0500 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.2374 10.2730 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.2374 10.4712 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.2372 13.2391 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.2367 10.6104 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.2367 11.3503 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.2367 10.8753 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.2365 10.5670 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.2361 10.2840 sec/batch\n",
      "Validation loss: 1.15663 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.2372 11.8705 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.2371 10.2107 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.2372 10.3981 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.2372 11.0149 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.2371 10.2787 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.2371 11.2217 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.2373 10.0615 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.2374 11.2310 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.2374 10.4534 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.2375 12.8056 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.2378 10.1977 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.2379 10.4255 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.2378 10.7473 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.2380 10.4107 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.2379 10.3529 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.2381 14.8086 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.2381 10.2227 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.2383 10.4923 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.2384 10.2334 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.2382 13.1059 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.2380 11.0519 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.2378 10.6655 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.2378 10.5024 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.2377 11.4851 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.2377 10.1304 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.2376 11.2104 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.2375 11.3849 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.2375 11.7009 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.2373 11.2853 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.2374 10.4987 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.2376 10.9659 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.2376 9.9349 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.2375 9.9775 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.2375 9.9066 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.2374 9.9019 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.2374 10.0062 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.2375 9.9356 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.2379 9.9500 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.2379 9.9363 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.2379 10.0544 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.2379 9.8848 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.2377 9.9258 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.2379 10.0477 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.2378 9.9563 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.2378 10.0482 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.2377 10.8008 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.2376 10.2995 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.2378 11.9313 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.3435 10.4100 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.2969 11.5871 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.2793 10.2282 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.2692 10.6973 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.2581 10.2881 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.2467 12.1993 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.2479 10.3727 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.2438 12.0450 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.2428 11.4495 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.2417 11.1511 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.2388 10.2309 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.2376 10.1569 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.2375 9.9419 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.2378 10.4009 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.2370 16.7676 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.2348 14.3105 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.2349 12.2178 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.2357 10.4827 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.2354 11.2555 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.2365 10.7286 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.2359 9.9713 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.2359 9.9757 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.2353 10.6254 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.2353 12.3440 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.2353 10.6708 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.2335 10.2964 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.2325 10.2544 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.2330 10.1068 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.2331 10.4790 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.2337 10.0129 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.2330 9.9961 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.2318 9.9873 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.2319 10.0161 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.2319 10.0042 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.2317 10.6137 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.2316 10.1316 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.2307 10.0654 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.2295 10.7748 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.2284 10.1230 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.2280 10.0170 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.2274 10.1321 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.2283 10.0207 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.2282 9.9695 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.2275 9.9907 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.2274 9.9993 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.2267 10.1672 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.2264 10.2362 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.2260 10.3699 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.2260 10.0297 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.2263 9.9652 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.2258 9.9858 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.2265 9.9197 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.2264 10.0818 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.2267 9.9721 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.2264 9.9745 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.2264 10.0664 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.2265 9.9616 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.2264 9.9796 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.2259 10.2014 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.2266 10.7770 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.2265 12.1961 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.2272 14.1164 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.2274 10.6776 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.2274 11.2416 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.2273 11.8668 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.2273 10.6327 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.2276 10.7337 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.2273 10.6280 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.2274 10.7307 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.2273 10.5741 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.2280 10.9124 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.2282 10.8094 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.2287 11.3952 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.2283 10.3461 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.2282 9.9737 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.2284 9.9155 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.2283 9.9237 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.2281 9.8691 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.2275 9.9050 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.2275 9.8860 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.2271 9.8970 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.2271 9.8914 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.2266 9.8714 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.2266 9.8288 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.2264 9.9835 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.2263 9.9200 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.2262 9.9422 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.2259 9.8657 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.2257 9.9781 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.2257 9.8727 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.2255 9.9626 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.2254 9.9297 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.2251 9.8527 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.2248 9.9679 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.2245 9.8776 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.2246 9.8511 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.2246 9.9287 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.2242 9.9090 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.2238 9.9276 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.2235 10.3496 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.2234 11.8794 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.2232 12.7903 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.2231 4215.0250 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.2230 708.1756 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.2229 10.2808 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.2227 10.6250 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.2226 11.2620 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.2226 17.7403 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.2225 14.0315 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.2225 26.4999 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.2223 22.4881 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.2223 22.8290 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.2223 14.5736 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.2222 17.6476 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.2219 16.1636 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.2217 17.5448 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.2218 14.8494 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.2217 14.9531 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.2216 14.7266 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.2216 16.7905 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.2214 14.2956 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.2211 15.2888 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.2207 19.7449 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.2207 19.7355 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.2206 22.6518 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.2202 17.9187 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.2202 16.4857 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.2203 16.3353 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.2201 15.9664 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.2198 16.0019 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.2194 16.7007 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.2192 16.0637 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.2193 16.1651 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.2193 18.0415 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.2192 15.9464 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.2193 16.0385 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.2195 15.6634 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.2196 15.7644 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.2196 20.1039 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.2197 18.4724 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.2199 18.6919 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.2200 13.3448 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.2199 15.7529 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.2202 13.8383 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.2201 16.3715 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.2203 18.6493 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.2204 20.9960 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.2206 17.4197 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.2206 20.6866 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.2205 21.5437 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.2203 22.3435 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.2201 14.3052 sec/batch\n",
      "Validation loss: 1.14607 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.2213 15.2611 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.2213 17.1467 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.2213 16.8333 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.2213 17.8117 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.2213 17.5123 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.2213 15.7365 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.2211 18.5525 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.2213 16.2217 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.2215 21.5408 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.2214 21.3778 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.2214 25.8849 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.2214 21.1682 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.2214 23.2874 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.2214 23.7449 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.2215 24.3996 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.2220 24.1097 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.2220 21.5736 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.2220 15.7055 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.2219 22.0447 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.2218 20.6637 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.2220 17.7427 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.2220 23.5650 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.2221 22.6102 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.2219 20.9778 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.2219 25.6912 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.2220 23.1302 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.3317 29.4358 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.2882 21.4325 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.2729 19.9055 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.2654 27.1223 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.2544 27.2835 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.2435 16.9545 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.2421 18.9415 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.2389 17.1666 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.2383 17.7170 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.2374 20.7419 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.2335 18.7108 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.2325 15.0149 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.2321 19.8432 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.2321 19.1501 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.2312 15.2240 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.2297 15.0262 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.2297 14.3606 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.2306 15.8971 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.2297 16.9132 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.2300 20.5631 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.2290 18.6558 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.2291 16.2651 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.2282 14.2461 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.2282 13.8750 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.2281 15.8578 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.2264 19.3401 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.2253 23.5855 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.2257 16.6914 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.2258 21.2309 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.2256 15.8477 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.2247 20.3908 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.2237 15.8665 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.2237 22.3696 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.2236 22.5563 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.2232 23.0080 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.2229 25.8773 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.2221 21.1570 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.2208 20.7187 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.2198 22.0709 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.2194 23.2660 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.2188 16.3870 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.2194 20.9001 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.2192 19.1167 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.2184 23.8002 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.2184 21.1551 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.2177 15.7310 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.2172 18.4648 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.2170 22.1810 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.2170 21.7883 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.2172 14.9624 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.2166 15.6132 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.2172 15.8556 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.2173 21.5695 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.2174 16.8117 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.2171 15.1431 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.2171 16.9211 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.2174 13.9873 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.2171 14.2318 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.2167 14.0562 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.2172 14.2821 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.2171 15.6873 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.2177 15.7122 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.2180 17.0807 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.2182 14.3738 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.2181 15.6112 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.2181 14.8176 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.2185 13.9458 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.2183 14.1749 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.2184 13.9814 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.2183 15.2332 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.2187 18.1349 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.2189 16.0318 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.2193 16.0809 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.2189 22.8765 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.2190 20.6189 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.2191 24.7215 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.2190 21.7417 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.2188 16.2003 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.2181 17.2820 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.2181 14.6672 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.2176 14.6977 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.2176 14.3307 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.2171 14.6371 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.2170 14.5344 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.2168 14.4872 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.2168 14.2107 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.2166 14.5462 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.2164 15.1351 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.2161 14.3909 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.2161 14.4638 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.2160 14.3538 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.2159 14.3580 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.2155 14.5986 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.2151 14.5789 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.2149 14.6639 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.2150 15.1718 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.2150 14.2380 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.2147 14.6974 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.2143 14.5522 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.2139 16.3286 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.2139 14.8914 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.2137 14.3244 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.2135 14.3530 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.2134 14.4419 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.2132 16.2644 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.2131 14.8482 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.2130 14.4926 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.2130 14.3777 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.2129 14.4264 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.2129 14.5419 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.2126 14.5993 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.2126 14.4658 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.2125 15.3181 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.2124 17.2321 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.2121 14.5994 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.2118 14.4948 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.2118 14.6246 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.2118 14.5133 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.2117 14.5063 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.2116 14.4984 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.2116 14.4440 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.2111 14.4207 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.2107 15.7100 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.2107 14.4649 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.2106 14.4293 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.2102 14.6650 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.2103 14.5547 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.2102 14.5178 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.2101 15.5203 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.2097 14.6179 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.2093 14.3071 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.2092 14.5494 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.2093 14.4254 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.2093 16.1093 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.2093 14.5796 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.2093 15.7389 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.2095 14.4799 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.2096 14.4064 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.2095 14.6384 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.2095 14.5459 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.2098 14.5423 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.2098 14.6025 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.2098 14.6151 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.2099 14.3847 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.2098 14.7528 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.2100 14.4343 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.2100 14.3228 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.2101 14.4456 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.2102 14.3137 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.2101 14.6233 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.2099 15.0715 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.2097 14.6753 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.2098 14.9624 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.2098 16.2454 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.2097 14.6560 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.2097 14.2290 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.2097 14.4356 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.2096 14.4700 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.2094 16.4580 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.2095 14.4985 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.2097 14.3679 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.2097 14.4379 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.2096 14.5950 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.2096 15.4802 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.2096 14.3957 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.2096 16.5002 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.2097 22.2408 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.2100 18.3961 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.2101 17.0520 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.2101 14.7268 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.2100 14.3973 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.2099 14.5794 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.2101 14.4871 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.2100 14.5737 sec/batch\n",
      "Validation loss: 1.13481 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.2112 17.6600 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.2111 14.4886 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.2110 14.5435 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.2112 14.8558 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.3147 15.7201 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.2617 14.9349 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.2510 15.4734 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.2436 18.2696 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.2344 19.9968 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.2221 18.8191 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.2218 19.6903 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.2193 19.3301 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.2170 19.1366 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.2172 22.4236 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.2141 18.5553 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.2140 20.1397 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.2144 18.5294 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.2148 18.3786 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.2137 15.9782 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.2121 20.5211 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.2124 15.3143 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.2131 17.3738 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.2129 16.1411 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.2145 14.7845 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.2138 15.8788 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.2141 16.0221 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.2133 14.7622 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.2137 14.9441 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.2134 17.1731 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.2117 15.0620 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.2104 16.4452 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.2112 14.6853 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.2113 15.6189 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.2118 15.2962 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.2113 16.8499 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.2103 19.2006 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.2101 14.9865 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.2099 14.9044 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.2094 14.7831 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.2094 14.7958 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.2086 14.7553 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.2073 16.7327 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.2062 16.3013 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.2058 15.0834 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.2052 18.8778 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.2059 15.6948 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.2057 19.7393 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.2048 17.7080 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.2051 16.9634 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.2046 19.8733 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.2042 22.7298 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.2037 19.7560 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.2038 21.7486 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.2038 22.7125 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.2033 21.7249 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.2038 17.7364 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.2037 18.7685 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.2039 20.7564 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.2037 19.0070 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.2036 20.7629 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.2038 17.5771 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.2037 18.2058 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.2033 18.9934 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.2040 14.7576 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.2039 14.4916 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.2045 14.5505 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.2048 15.2484 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.2048 14.3286 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.2048 16.4046 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.2048 18.6991 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.2050 14.7188 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.2049 15.6473 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.2050 17.1012 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.2048 20.5721 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.2054 14.5851 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.2056 16.6006 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.2061 15.6274 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.2057 16.1137 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.2056 15.7681 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.2058 14.9835 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.2058 15.1532 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.2056 15.9786 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.2050 18.0676 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.2049 21.8091 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.2045 20.1747 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.2044 19.3024 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.2039 20.8595 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.2038 18.9205 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.2034 18.7974 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.2032 19.1022 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.2030 18.5093 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.2029 19.1465 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.2025 22.2880 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.2026 18.6415 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.2024 18.9110 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.2023 20.4796 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.2020 19.1941 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.2017 18.3881 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.2015 14.5517 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.2017 14.4906 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.2017 14.4459 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.2013 14.4606 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.2010 14.5896 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.2007 14.3185 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.2007 14.3501 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.2005 14.2986 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.2005 16.2757 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.2003 14.2645 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.2003 14.1486 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.2001 14.2413 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.2000 14.5544 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.2000 14.2803 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.1999 15.3034 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.1999 14.3605 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.1998 14.1360 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.1997 14.2687 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.1996 14.2192 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.1994 14.1876 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.1991 16.1680 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.1988 14.8535 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.1988 16.6570 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.1988 19.0848 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.1986 14.1885 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.1986 14.2962 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.1985 14.2049 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.1981 14.3317 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.1977 14.3703 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.1977 15.2884 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.1976 14.0262 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.1972 14.0836 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.1972 14.0795 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.1972 14.3886 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.1971 14.2891 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.1967 14.2698 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.1964 14.2083 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.1963 14.5389 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.1965 16.0918 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.1964 14.2232 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.1964 14.2766 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.1965 14.2763 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.1966 14.4012 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.1967 14.0856 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.1968 17.7359 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.1968 14.4071 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.1972 14.1594 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.1973 14.4657 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.1972 14.2485 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.1974 14.4718 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.1974 14.3742 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.1975 15.2715 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.1976 14.0377 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.1978 14.3291 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.1979 14.1221 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.1977 14.0791 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.1975 14.1631 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.1974 14.2476 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.1974 14.3298 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.1974 14.6289 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.1974 15.2783 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.1974 13.8978 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.1973 13.9493 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.1972 14.3901 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.1970 14.1337 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.1971 14.8100 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.1973 15.5993 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.1973 13.9737 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.1972 13.9190 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.1973 13.8932 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.1972 14.0703 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.1973 13.9895 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.1974 14.1203 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.1977 13.9780 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.1978 13.9007 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.1978 13.9210 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.1977 13.9057 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.1977 14.0229 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.1979 13.9825 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.1979 14.1052 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.1980 13.9125 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.1978 14.8084 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.1978 13.8190 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.1980 13.9618 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.3098 13.9695 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.2632 14.7871 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.2430 16.3585 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.2359 14.7476 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.2219 14.1173 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.2112 14.0057 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.2111 14.9730 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.2087 14.1047 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.2073 13.9265 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.2061 13.9826 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.2029 14.0660 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.2020 13.8458 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.2024 14.0541 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.2023 14.3060 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.2010 14.1213 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.1990 13.8183 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.1995 14.0226 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.2008 13.9837 sec/batch\n",
      "Validation loss: 1.12772 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.2100 19.4919 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.2110 16.6669 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.2101 18.4734 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.2096 18.8420 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.2087 16.1158 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.2088 16.2986 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.2085 17.3779 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.2069 16.2374 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.2056 17.1899 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.2061 22.2455 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.2060 15.7828 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.2062 16.2275 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.2056 15.4113 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.2045 16.0835 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.2042 17.0571 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.2042 15.7635 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.2036 14.5411 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.2030 15.8096 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.2021 15.4883 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.2008 15.1193 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.1996 14.5298 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.1993 14.3431 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.1985 14.7323 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.1993 15.6372 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.1990 16.0915 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.1982 14.9155 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.1983 15.2118 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.1976 17.0946 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.1970 15.4370 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.1964 14.7815 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.1964 17.0075 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.1966 14.9790 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.1961 14.4518 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.1964 14.2132 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.1962 14.8972 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.1962 16.3806 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.1959 22.6609 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.1960 20.1057 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.1961 14.7087 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.1957 15.6070 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.1951 15.8343 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.1958 20.2768 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.1957 21.1297 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.1963 16.7459 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.1967 15.1240 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.1965 14.4465 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.1966 17.0558 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.1966 14.4359 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.1968 16.5797 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.1965 16.7677 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.1965 19.6086 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.1964 15.3183 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.1969 15.8609 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.1970 15.1997 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.1974 14.8896 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.1971 18.0242 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.1969 15.9906 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.1969 14.8905 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.1967 19.5990 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.1967 19.5438 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.1961 19.3042 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.1961 21.6598 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.1956 21.8036 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.1955 19.6288 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.1950 18.8966 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.1949 18.0176 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.1944 14.3667 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.1943 13.3671 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.1941 13.1425 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.1939 13.2381 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.1935 13.3110 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.1936 13.2199 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.1933 13.3640 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.1932 13.1856 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.1928 14.1550 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.1925 13.3706 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.1922 13.2848 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.1923 13.2726 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.1924 13.1063 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.1919 13.4934 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.1916 17.9044 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.1914 21.4714 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.1913 16.3410 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.1912 23.6730 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.1911 17.0105 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.1910 19.4320 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.1909 18.0141 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.1906 17.7761 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.1906 17.7637 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.1906 19.0740 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.1903 17.1554 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.1904 16.9123 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.1902 19.8798 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.1901 17.4364 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.1900 15.5293 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.1899 14.6836 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.1896 16.7022 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.1893 14.7824 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.1894 13.4139 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.1893 16.8801 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.1891 19.5102 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.1891 13.8791 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.1890 14.4263 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.1887 14.1285 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.1883 18.3605 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.1883 17.4071 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.1882 15.5718 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.1879 15.3445 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.1878 15.0185 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.1878 14.3156 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.1876 13.4388 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.1872 19.2920 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.1869 15.7172 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.1867 16.4763 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.1868 22.0129 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.1868 20.2233 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.1868 20.3983 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.1868 16.0683 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.1870 19.8267 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.1872 18.0362 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.1872 16.5869 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.1872 17.1775 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.1875 13.7077 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.1876 17.7539 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.1875 16.6564 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.1877 15.3637 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.1877 14.5229 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.1879 19.5104 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.1879 13.7407 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.1881 16.3839 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.1883 18.9520 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.1882 16.6416 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.1880 14.5477 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.1878 15.6702 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.1879 16.4618 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.1878 14.5921 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.1878 18.0265 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.1877 15.6374 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.1878 14.6408 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.1877 13.8621 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.1875 13.8189 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.1876 14.8101 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.1878 13.7386 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.1879 13.6799 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.1878 13.5843 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.1877 13.6082 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.1878 13.7983 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.1877 13.6369 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.1878 13.4868 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.1881 13.5594 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.1882 13.7624 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.1883 15.9696 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.1882 17.9602 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.1881 19.9284 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.1883 23.6697 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.1883 19.5507 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.1883 21.3487 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.1882 18.8982 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.1881 15.8298 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.1883 16.4634 sec/batch\n",
      "Validation loss: 1.12299 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONE HOT\n",
      "(1, 1)\n",
      "Tensor(\"strided_slice:0\", shape=(1, 1), dtype=int32)\n",
      "83\n",
      "(1, 1, 83)\n",
      "Tensor(\"strided_slice_1:0\", shape=(1, 1, 10), dtype=float32)\n",
      "FIRST\n",
      "\n",
      "\n",
      "1\n",
      "Tensor(\"Squeeze:0\", shape=(1, 83), dtype=float32)\n",
      "\n",
      "\n",
      "\n",
      "OUTPUT\n",
      "\n",
      "\n",
      "1\n",
      "Tensor(\"rnn/multi_rnn_cell/cell_1/dropout/mul:0\", shape=(1, 512), dtype=float32)\n",
      "\n",
      "\n",
      "\n",
      "SECOND\n",
      "\n",
      "\n",
      "(1, 512)\n",
      "(1, 512)\n",
      "THIRD\n",
      "\n",
      "\n",
      "(1, 83)\n",
      "(1, 83)\n",
      "(1, 1, 83)\n",
      "Tensor(\"Reshape_4:0\", shape=(1,), dtype=float32)\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "End\n",
      "\n",
      "\n",
      "{'final_state': (LSTMStateTuple(c=<tf.Tensor 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/add_1:0' shape=(1, 512) dtype=float32>, h=<tf.Tensor 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/mul_2:0' shape=(1, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/add_1:0' shape=(1, 512) dtype=float32>, h=<tf.Tensor 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/mul_2:0' shape=(1, 512) dtype=float32>)), 'num_steps': 1, 'lstm_size': 512, 'softmax_w': <tensorflow.python.ops.variables.Variable object at 0x11c8bda58>, 'cell': <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object at 0x1292f9a90>, 'keep_prob': <tf.Tensor 'keep_prob:0' shape=<unknown> dtype=float32>, 'logits': <tf.Tensor 'add:0' shape=(1, 83) dtype=float32>, 'learning_rate': 0.001, 'loss': <tf.Tensor 'Reshape_4:0' shape=(1,) dtype=float32>, 'drop': <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.DropoutWrapper object at 0x1292edf28>, 'export_nodes': ['inputs', 'targets', 'initial_state', 'final_state', 'keep_prob', 'cost', 'preds', 'optimizer'], 'tvars': [<tensorflow.python.ops.variables.Variable object at 0x11c8cdeb8>, <tensorflow.python.ops.variables.Variable object at 0x11c8d4390>, <tensorflow.python.ops.variables.Variable object at 0x12b8389b0>, <tensorflow.python.ops.variables.Variable object at 0x12b8386a0>, <tensorflow.python.ops.variables.Variable object at 0x11c8bda58>, <tensorflow.python.ops.variables.Variable object at 0x12b838d30>], 'grad_clip': 5, 'cost': <tf.Tensor 'Mean:0' shape=() dtype=float32>, 'train_op': <tensorflow.python.training.adam.AdamOptimizer object at 0x11c77fb70>, 'grads': [<tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_0:0' shape=(595, 2048) dtype=float32>, <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_1:0' shape=(2048,) dtype=float32>, <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_2:0' shape=(1024, 2048) dtype=float32>, <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_3:0' shape=(2048,) dtype=float32>, <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_4:0' shape=(512, 83) dtype=float32>, <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_5:0' shape=(83,) dtype=float32>], 'outputs': [<tf.Tensor 'rnn/multi_rnn_cell/cell_1/dropout/mul:0' shape=(1, 512) dtype=float32>], 'initial_state': (LSTMStateTuple(c=<tf.Tensor 'zeros:0' shape=(1, 512) dtype=float32>, h=<tf.Tensor 'zeros_1:0' shape=(1, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'zeros_2:0' shape=(1, 512) dtype=float32>, h=<tf.Tensor 'zeros_3:0' shape=(1, 512) dtype=float32>)), 'preds': <tf.Tensor 'predictions:0' shape=(1, 83) dtype=float32>, '_': <tf.Tensor 'global_norm/global_norm:0' shape=() dtype=float32>, 'sampling': True, 'Graph': <class '__main__.Graph'>, 'inputs': <tf.Tensor 'inputs:0' shape=(1, 1) dtype=int32>, 'lstm': <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x1292edb70>, 'output': <tf.Tensor 'Reshape:0' shape=(1, 512) dtype=float32>, 'y_reshaped': <tf.Tensor 'Reshape_1:0' shape=(1, 83) dtype=float32>, 'state': (LSTMStateTuple(c=<tf.Tensor 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/add_1:0' shape=(1, 512) dtype=float32>, h=<tf.Tensor 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/mul_2:0' shape=(1, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/add_1:0' shape=(1, 512) dtype=float32>, h=<tf.Tensor 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/mul_2:0' shape=(1, 512) dtype=float32>)), 'targets': <tf.Tensor 'targets:0' shape=(1, 1) dtype=int32>, 'x_one_hot': <tf.Tensor 'one_hot:0' shape=(1, 1, 83) dtype=float32>, 'seq_output': <tf.Tensor 'concat:0' shape=(1, 512) dtype=float32>, 'batch_size': 1, 'rnn_inputs': [<tf.Tensor 'Squeeze:0' shape=(1, 83) dtype=float32>], 'y_one_hot': <tf.Tensor 'one_hot_1:0' shape=(1, 1, 83) dtype=float32>, 'optimizer': <tf.Operation 'Adam' type=NoOp>, 'num_classes': 83, 'softmax_b': <tensorflow.python.ops.variables.Variable object at 0x12b838d30>, 'num_layers': 2}\n",
      "Graph(inputs=<tf.Tensor 'inputs:0' shape=(1, 1) dtype=int32>, targets=<tf.Tensor 'targets:0' shape=(1, 1) dtype=int32>, initial_state=(LSTMStateTuple(c=<tf.Tensor 'zeros:0' shape=(1, 512) dtype=float32>, h=<tf.Tensor 'zeros_1:0' shape=(1, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'zeros_2:0' shape=(1, 512) dtype=float32>, h=<tf.Tensor 'zeros_3:0' shape=(1, 512) dtype=float32>)), final_state=(LSTMStateTuple(c=<tf.Tensor 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/add_1:0' shape=(1, 512) dtype=float32>, h=<tf.Tensor 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/mul_2:0' shape=(1, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/add_1:0' shape=(1, 512) dtype=float32>, h=<tf.Tensor 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/mul_2:0' shape=(1, 512) dtype=float32>)), keep_prob=<tf.Tensor 'keep_prob:0' shape=<unknown> dtype=float32>, cost=<tf.Tensor 'Mean:0' shape=() dtype=float32>, preds=<tf.Tensor 'predictions:0' shape=(1, 83) dtype=float32>, optimizer=<tf.Operation 'Adam' type=NoOp>)\n",
      "Farth....\n",
      " \"A different matter, they work is something but a commusiration with\n",
      "him. And there was no distant of the passage to the consciousness\n",
      "of in the poor, or which they should have been despaired, and wishe\n",
      "to her to step and dististen her the princess...\"\n",
      "\n",
      "\"You don't know.\"\n",
      "\n",
      "\"There were a master.\"\n",
      "\n",
      "A supressible pleasure had been at the table. She came to step to\n",
      "her, and say in the same sincerition, which when he stopped. The\n",
      "difficulties of her soul would be a part of the change that his brother\n",
      "stoop talking of his head with taking. \"That's not it with my words and\n",
      "son in the service of a propise of me that you would be doubts. I am\n",
      "sincele to me,\" Alexey Alexandrovitch said already something. \"Ah,\n",
      "it's a good part of the stable in the cormection of him, that's\n",
      "the carriage in the compressing trouser in them.\n",
      "\n",
      "They might to say, I won't think it is?\" he said in the princess. \"It's\n",
      "a chill of such a privote can and thinking, that is something in\n",
      "the morning, that's the meanow of the service of anything in all this were\n",
      "beer for the massing the prince, who will be only so much of the barthest\n",
      "of the most.\"\n",
      "\n",
      "\"If I want to say that the contrary, they don't thought..\n",
      "\n",
      "\n",
      "\"Challen you she say, at the storm that I can be all the steps in the chair.\n",
      "I are all a mustre. A coutt is not in a far outside a secrity about\n",
      "it?\"\n",
      "\n",
      "\"No, and he suddenly to be in her, and he would have to be sure as\n",
      "the couriers.\"\n",
      "\n",
      "\"Oh, what are you a lifin and me? I'd see his book at it. I would not be\n",
      "a sour for this at that thing. To me that the princess,\" she would say\n",
      "that it seemed as though she had to see the soling instant.\n",
      "\n",
      "\"Well,\" she said, getting up, and stood into the pleasure in the dinger,\n",
      "and to him a memory on the same, to the bridge of the course to the position\n",
      "that he was sistening. She had never spent the court of taking it to a married\n",
      "what there is to see to the country, because in his househelfored. And\n",
      "the call of the more had an effort and his side word to him to be\n",
      "stirr\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/i3560_l512_v1.123.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
